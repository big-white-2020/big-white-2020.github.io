[{"title":"Spark面试80连击(上)","date":"2021-07-12T04:22:00.000Z","url":"/2021/07/12/Spark%E9%9D%A2%E8%AF%9580%E8%BF%9E%E5%87%BB(%E4%B8%8A)/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark消费 Kafka，分布式的情况下，如何保证消息的顺序?Kafka 分布式的单位是 Partition。如何保证消息有序，需要分几个情况讨论。 同一个 Partition 用一个 write ahead log 组织，所以可以保证 FIFO 的顺序。 不同 Partition 之间不能保证顺序。 但是绝大多数用户都可以通过 message key 来定义，因为同一个 key 的 message 可以保证只发送到同一个 Partition。 比如说 key 是 user id，table row id 等等，所以同一个 user 或者同一个 record 的消息永远只会发送到同一个 Partition上，保证了同一个 user 或 record 的顺序。 当然，如果你有 key skewness 就有些麻烦，需要特殊处理。 实际情况中: （1）不关注顺序的业务大量存在；（2）队列无序不代表消息无序。 第（2）条的意思是说: 我们不保证队列的全局有序，但可以保证消息的局部有序。举个例子: 保证来自同1个 order id 的消息，是有序的！ Kafka 中发送1条消息的时候，可以指定(topic, partition, key) 3个参数。partiton 和 key 是可选的。如果你指定了 partition，那就是所有消息发往同1个 partition，就是有序的。并且在消费端，Kafka 保证，1个 partition 只能被1个 consumer 消费。或者你指定 key（比如 order id），具有同1个 key 的所有消息，会发往同1个 partition。也是有序的。 参考： 对于 Spark 中的数据倾斜问题你有什么好的方案？简单一句: Spark 数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义 Partitioner，使用 Map 侧 Join 代替 Reduce 侧 Join（内存表合并），给倾斜 Key 加上随机前缀等。 什么是数据倾斜 对 Spark/Hadoop 这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。数据倾斜指的是，并行处理的数据集中，某一部分（如 Spark 或 Kafka 的一个 Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈（木桶效应）。 数据倾斜是如何造成的 在 Spark 中，同一个 Stage 的不同 Partition 可以并行处理，而具有依赖关系的不同 Stage 之间是串行处理的。假设某个 Spark Job 分为 Stage 0和 Stage 1两个 Stage，且 Stage 1依赖于 Stage 0，那 Stage 0完全处理结束之前不会处理Stage 1。而 Stage 0可能包含 N 个 Task，这 N 个 Task 可以并行进行。如果其中 N-1个 Task 都在10秒内完成，而另外一个 Task 却耗时1分钟，那该 Stage 的总时间至少为1分钟。换句话说，一个 Stage 所耗费的时间，主要由最慢的那个 Task 决定。由于同一个 Stage 内的所有 Task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同 Task 之间耗时的差异主要由该 Task 所处理的数据量决定。 具体解决方案 ： 调整并行度分散同一个 Task 的不同 Key: Spark 在做 Shuffle 时，默认使用 HashPartitioner对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。图中左边绿色框表示 kv 样式的数据，key 可以理解成 name。可以看到 Task0 分配了许多的 key，调整并行度，多了几个 Task，那么每个 Task 处理的数据量就分散了。 自定义Partitioner: 使用自定义的 Partitioner（默认为 HashPartitioner），将原本被分配到同一个 Task 的不同 Key 分配到不同 Task，可以拿上图继续想象一下，通过自定义 Partitioner 可以把原本分到 Task0 的 Key 分到 Task1，那么 Task0 的要处理的数据量就少了。 将 Reduce side（侧） Join 转变为 Map side（侧） Join: 通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。可以看到 RDD2 被加载到内存中了。 为 skew 的 key 增加随机前/后缀: 为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜 Key 如何加前缀，都能与之正常 Join。 大表随机添加 N 种随机前缀，小表扩大 N 倍: 如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大（很难一个 Key 一个 Key 都加上后缀）。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大 N 倍），可以看到 RDD2 扩大了 N 倍了，再和加完前缀的大数据做笛卡尔积。 你所理解的 Spark 的 shuffle 过程？Spark shuffle 处于一个宽依赖，可以实现类似混洗的功能，将相同的 Key 分发至同一个 Reducer上进行处理。 Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？在我们的开发过程中，能避免则尽可能避免使用 reduceByKey、join、distinct、repartition 等会进行 shuffle 的算子，尽量使用 map 类的非 shuffle 算子。这样的话，没有 shuffle 操作或者仅有较少 shuffle 操作的 Spark 作业，可以大大减少性能开销。 spark on yarn 作业执行流程，yarn-client 和 yarn cluster 有什么区别Spark On Yarn 的优势 Spark 支持资源动态共享，运行于 Yarn 的框架都共享一个集中配置好的资源池 可以很方便的利用 Yarn 的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 Yarn 可以自由地选择 executor 数量 Yarn 是唯一支持 Spark 安全的集群管理器，使用 Yarn，Spark 可以运行于 Kerberized Hadoop 之上，在它们进程之间进行安全认证 yarn-client 和 yarn cluster 的异同 1. 从广义上讲，yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出。2. 从深层次的含义讲，yarn-cluster 和 yarn-client 模式的区别其实就是 Application Master 进程的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的 container 通信来调度他们工作，也就是说 Client 不能离开。 Spark为什么快，Spark SQL 一定比 Hive 快吗Spark SQL 比 Hadoop Hive 快，是有一定条件的，而且不是 Spark SQL 的引擎比 Hive 的引擎快，相反，Hive 的 HQL 引擎还比 Spark SQL 的引擎更快。其实，关键还是在于 Spark 本身快。 消除了冗余的 HDFS 读写: Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。 如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。 消除了冗余的 MapReduce 阶段: Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。 而 Spark 基于 RDD 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。 JVM 的优化: Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。 而 Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。 每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多少。 记住一种反例 考虑一种极端查询: 这个查询只有一次 shuffle 操作，此时，也许 Hive HQL 的运行时间也许比 Spark 还快，反正 shuffle 完了都会落一次盘，或者都不落盘。 结论 Spark 快不是绝对的，但是绝大多数，Spark 都比 Hadoop 计算要快。这主要得益于其对 mapreduce 操作的优化以及对 JVM 使用的优化。 RDD, DAG, Stage怎么理解？DAG Spark 中使用 DAG 对 RDD 的关系进行建模，描述了 RDD 的依赖关系，这种关系也被称之为 lineage（血缘），RDD 的依赖关系使用 Dependency 维护。DAG 在 Spark 中的对应的实现为 DAGScheduler。 RDD RDD 是 Spark 的灵魂，也称为弹性分布式数据集。一个 RDD 代表一个可以被分区的只读数据集。RDD 内部可以有许多分区(partitions)，每个分区又拥有大量的记录(records)。Rdd的五个特征：1. dependencies: 建立 RDD 的依赖关系，主要 RDD 之间是宽窄依赖的关系，具有窄依赖关系的 RDD 可以在同一个 stage 中进行计算。2. partition: 一个 RDD 会有若干个分区，分区的大小决定了对这个 RDD 计算的粒度，每个 RDD 的分区的计算都在一个单独的任务中进行。3. preferedlocations: 按照“移动数据不如移动计算”原则，在 Spark 进行任务调度的时候，优先将任务分配到数据块存储的位置。4. compute: Spark 中的计算都是以分区为基本单位的，compute 函数只是对迭代器进行复合，并不保存单次计算的结果。5. partitioner: 只存在于（K,V）类型的 RDD 中，非（K,V）类型的 partitioner 的值就是 None。 RDD 的算子主要分成2类，action 和 transformation。这里的算子概念，可以理解成就是对数据集的变换。action 会触发真正的作业提交，而 transformation 算子是不会立即触发作业提交的。每一个 transformation 方法返回一个新的 RDD。只是某些 transformation 比较复杂，会包含多个子 transformation，因而会生成多个 RDD。这就是实际 RDD 个数比我们想象的多一些 的原因。通常是，当遇到 action 算子时会触发一个job的提交，然后反推回去看前面的 transformation 算子，进而形成一张有向无环图。 Stage 在 DAG 中又进行 stage 的划分，划分的依据是依赖是否是 shuffle 的，每个 stage 又可以划分成若干 task。接下来的事情就是 driver 发送 task 到 executor，executor 自己的线程池去执行这些 task，完成之后将结果返回给 driver。action 算子是划分不同 job 的依据。 RDD 如何通过记录更新的方式容错RDD 的容错机制实现分布式数据集容错方法有两种: 1. 数据检查点 2. 记录更新。 RDD 采用记录更新的方式：记录所有更新点的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个 RDD 的变换序列（血统 lineage）存储下来；变换序列指，每个 RDD 都包含了它是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称“血统”容错。 宽依赖、窄依赖怎么理解？窄依赖指的是每一个 parent RDD 的 partition 最多被子 RDD 的一个 partition 使用（一子一亲）。 宽依赖指的是多个子 RDD 的 partition 会依赖同一个 parent RDD的 partition（多子一亲）。 RDD 作为数据结构，本质上是一个只读的分区记录集合。一个 RDD 可以包含多个分区，每个分区就是一个 dataset 片段。RDD 可以相互依赖。 首先，窄依赖可以支持在同一个 cluster node上，以 pipeline 形式执行多条命令（也叫同一个 stage 的操作），例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。 其次，则是从失败恢复的角度考虑。窄依赖的失败恢复更有效，因为它只需要重新计算丢失的 parent partition 即可，而且可以并行地在不同节点进行重计算（一台机器太慢就会分配到多个节点进行），相反，宽依赖牵涉 RDD 各级的多个 parent partition。 Job 和 Task 怎么理解Job Spark 的 Job 来源于用户执行 action 操作（这是 Spark 中实际意义的 Job），就是从 RDD 中获取结果的操作，而不是将一个 RDD 转换成另一个 RDD 的 transformation 操作。 Task 一个 Stage 内，最终的 RDD 有多少个 partition，就会产生多少个 task。看一看图就明白了，可以数一数每个 Stage 有多少个 Task。 Spark 血统的概念RDD 的 lineage 记录的是粗颗粒度的特定数据转换（transformation）操作（filter, map, join etc.)行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了 Spark 的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。 任务的概念包含很多 task 的并行计算，可以认为是 Spark RDD 里面的 action，每个 action 的计算会生成一个 job。用户提交的 job 会提交给 DAGScheduler，job 会被分解成 Stage 和 Task。 容错方法Spark 选择记录更新的方式。但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建 RDD 的一系列变换序列（每个 RDD 都包含了他是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称血统容错）记录下来，以便恢复丢失的分区。lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。 相比其他系统的细颗粒度的内存数据更新级别的备份或者 LOG 机制，RDD 的 lineage 记录的是粗颗粒度的特定数据 transformation 操作行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。 Spark 粗粒度和细粒度如果问的是操作的粗细粒度，应该是，Spark 在错误恢复的时候，只需要粗粒度的记住 lineage，就可实现容错。 关于 Mesos 1. 粗粒度模式（Coarse-grained Mode）: 每个应用程序的运行环境由一个 dirver 和若干个 executor 组成，其中，每个 executor 占用若干资源，内部可运行多个 task（对应多少个 slot）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。举个例子，比如你提交应用程序时，指定使用5个 executor 运行你的应用程序，每个 executor 占用5GB内存和5个 CPU，每个 executor 内部设置了5个 slot，则 Mesos 需要先为 executor 分配资源并启动它们，之后开始调度任务。另外，在程序运行过程中，Mesos 的 master 和 slave 并不知道 executor 内部各个 task 的运行情况，executor 直接将任务状态通过内部的通信机制汇报给 driver，从一定程度上可以认为，每个应用程序利用 Mesos 搭建了一个虚拟集群自己使用。2. 细粒度模式（Fine-grained Mode）: 鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos 还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动 executor，但每个 executor 占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，Mesos 会为每个 executor 动态分配资源，每分配一些，便可以运行一个新任务，单个 Task 运行完之后可以马上释放对应的资源。每个 Task 会汇报状态给 Mesos slave 和 Mesos Master，便于更加细粒度管理和容错，这种调度模式类似于 MapReduce 调度模式，每个 task 完全独立，优点是便于资源控制和隔离，但缺点也很明显，短作业运行延迟大。 Spark中，每个 application 对应一个 SparkContext。对于 SparkContext 之间的调度关系，取决于 Spark 的运行模式。对 Standalone 模式而言，Spark Master 节点先计算集群内的计算资源能否满足等待队列中的应用对内存和 CPU 资源的需求，如果可以，则 Master 创建 Spark Driver，启动应用的执行。宏观上来讲，这种对应用的调度类似于 FIFO 策略。在 Mesos 和 Yarn 模式下，底层的资源调度系统的调度策略都是由 Mesos 和 Yarn 决定的。具体分类描述如下： Standalone 模式: 默认以用户提交 Applicaiton 的顺序来调度，即 FIFO 策略。 每个应用执行时独占所有资源。 如果有多个用户要共享集群资源，则可以使用参数 spark.cores.max 来配置应用在集群中可以使用的最大 CPU 核的数量。 如果不配置，则采用默认参数 spark.deploy.defaultCore 的值来确定。 Mesos 模式: 如果在 Mesos 上运行 Spark，用户想要静态配置资源的话，可以设置 spark.mesos.coarse 为 true，这样 Mesos 变为粗粒度调度模式。 然后可以设置 spark.cores.max 指定集群中可以使用的最大核数，与上面 Standalone 模式类似。 同时，在 Mesos 模式下，用户还可以设置参数 spark.executor.memory 来配置每个 executor 的内存使用量。 如果想使 Mesos 在细粒度模式下运行，可以通过 mesos://&lt;url-info&gt; 设置动态共享 CPU core 的执行模式。 在这种模式下，应用不执行时的空闲 CPU 资源得以被其他用户使用，提升了 CPU 使用率。 Spark优越性一、Spark 的5大优势：1. 更高的性能。因为数据被加载到集群主机的分布式内存中。数据可以被快速的转换迭代，并缓存用以后续的频繁访问需求。在数据全部加载到内存的情况下，Spark可以比Hadoop快100倍，在内存不够存放所有数据的情况下快hadoop10倍。2. 通过建立在Java,Scala,Python,SQL（应对交互式查询）的标准API以方便各行各业使用，同时还含有大量开箱即用的机器学习库。3. 与现有Hadoop 1和2.x(YARN)生态兼容，因此机构可以无缝迁移。4. 方便下载和安装。方便的shell（REPL: Read-Eval-Print-Loop）可以对API进行交互式的学习。5. 借助高等级的架构提高生产力，从而可以讲精力放到计算上。 二、MapReduce与Spark相比，有哪些异同点：1、基本原理上：（1） MapReduce：基于磁盘的大数据批量处理系统 （2）Spark：基于RDD(弹性分布式数据集)数据处理，显示将RDD数据存储到磁盘和内存中。2、模型上：（1） MapReduce可以处理超大规模的数据，适合日志分析挖掘等较少的迭代的长任务需求，结合了数据的分布式的计算。（2） Spark：适合数据的挖掘，机器学习等多轮迭代式计算任务。 Transformation和action是什么？区别？举几个常用方法**RDD 创建后就可以在 RDD 上进行数据处理。RDD 支持两种操作: 1. 转换（transformation）: 即从现有的数据集创建一个新的数据集 2. 动作（action）: 即在数据集上进行计算后，返回一个值给 Driver 程序 RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。 RDD 中所有的 Transformation 都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给 Driver 的 Action 时，这些 Transformation 才会真正运行。 这个设计让 Spark 更加有效的运行。 Spark作业提交流程是怎么样的 spark-submit 提交代码，执行 new SparkContext()，在 SparkContext 里构造 DAGScheduler 和 TaskScheduler。 TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。 Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。 Executor 启动后，会自己反向注册到 TaskScheduler 中。 所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。 每执行到一个 Action，就会创建一个 Job。 Job 会提交给 DAGScheduler。 DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。 TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。 Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。 (TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition) Spark streamning工作流程是怎么样的，和Storm比有什么区别Spark Streaming 与 Storm 都可以用于进行实时流计算。但是他们两者的区别是非常大的。其中区别之一，就是，Spark Streaming 和 Storm 的计算模型完全不一样，Spark Streaming 是基于 RDD 的，因此需要将一小段时间内的，比如1秒内的数据，收集起来，作为一个 RDD，然后再针对这个 batch 的数据进行处理。而 Storm 却可以做到每来一条数据，都可以立即进行处理和计算。因此，Spark Streaming 实际上严格意义上来说，只能称作准实时的流计算框架；而 Storm 是真正意义上的实时计算框架。此外，Storm 支持的一项高级特性，是 Spark Streaming 暂时不具备的，即 Storm 支持在分布式流式计算程序（Topology）在运行过程中，可以动态地调整并行度，从而动态提高并发处理能力。而 Spark Streaming 是无法动态调整并行度的。但是 Spark Streaming 也有其优点，首先 Spark Streaming 由于是基于 batch 进行处理的，因此相较于 Storm 基于单条数据进行处理，具有数倍甚至数十倍的吞吐量。此外，Spark Streaming 由于也身处于 Spark 生态圈内，因此Spark Streaming可以与Spark Core、Spark SQL，甚至是Spark MLlib、Spark GraphX进行无缝整合。流式处理完的数据，可以立即进行各种map、reduce转换操作，可以立即使用sql进行查询，甚至可以立即使用machine learning或者图计算算法进行处理。这种一站式的大数据处理功能和优势，是Storm无法匹敌的。因此，综合上述来看，通常在对实时性要求特别高，而且实时数据量不稳定，比如在白天有高峰期的情况下，可以选择使用Storm。但是如果是对实时性要求一般，允许1秒的准实时处理，而且不要求动态调整并行度的话，选择Spark Streaming是更好的选择。 参考： Spark sql你使用过没有，在哪个项目里面使用的离线 ETL 之类的，结合机器学习等 Spark 机器学习和 Spark 图计算接触过没有，举例说明你用它做过什么？Spark 提供了很多机器学习库，我们只需要填入数据，设置参数就可以用了。使用起来非常方便。另外一方面，由于它把所有的东西都写到了内部，我们无法修改其实现过程。要想修改里面的某个环节，还的修改源码，重新编译。比如 kmeans 算法，如果没有特殊需求，很方便。但是spark内部使用的两个向量间的距离是欧式距离。如果你想改为余弦或者马氏距离，就的重新编译源码了。Spark 里面的机器学习库都是一些经典的算法，这些代码网上也好找。这些代码使用起来叫麻烦，但是很灵活。Spark 有一个很大的优势，那就是 RDD。模型的训练完全是并行的。 Spark 的 ML 和 MLLib 两个包区别和联系 技术角度上，面向的数据集类型不一样: ML 的 API 是面向 Dataset 的（Dataframe 是 Dataset 的子集，也就是 Dataset[Row]）， mllib 是面对 RDD 的。 Dataset 和 RDD 有啥不一样呢？ Dataset 的底端是 RDD。 Dataset 对 RDD 进行了更深一层的优化，比如说有 sql 语言类似的黑魔法，Dataset 支持静态类型分析所以在 compile time 就能报错，各种 combinators（map，foreach 等）性能会更好，等等。 编程过程上，构建机器学习算法的过程不一样: ML 提倡使用 pipelines，把数据想成水，水从管道的一段流入，从另一端流出。 ML 是1.4比 Mllib 更高抽象的库，它解决如果简洁的设计一个机器学习工作流的问题，而不是具体的某种机器学习算法。 未来这两个库会并行发展。 Spark RDD是怎么容错的，基本原理是什么？一般来说，分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。 面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。 因此，Spark选择记录更新的方式。但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。 Lineage机制Lineage简介 相比其他系统的细颗粒度的内存数据更新级别的备份或者LOG机制，RDD的Lineage记录的是粗颗粒度的特定数据Transformation操作（如filter、map、join等）行为。当这个RDD的部分分区数据丢失时，它可以通过Lineage获取足够的信息来重新运算和恢复丢失的数据分区。因为这种粗颗粒的数据模型，限制了Spark的运用场合，所以Spark并不适用于所有高性能要求的场景，但同时相比细颗粒度的数据模型，也带来了性能的提升。 两种依赖关系 RDD在Lineage依赖方面分为两种：窄依赖(Narrow Dependencies)与宽依赖(Wide Dependencies,源码中称为Shuffle Dependencies)，用来解决数据容错的高效性。 窄依赖是指父RDD的每一个分区最多被一个子RDD的分区所用，表现为一个父RDD的分区对应于一个子RDD的分区或多个父RDD的分区对应于一个子RDD的分区，也就是说一个父RDD的一个分区不可能对应一个子RDD的多个分区。 1个父RDD分区对应1个子RDD分区，这其中又分两种情况： 1个子RDD分区对应1个父RDD分区（如map、filter等算子），1个子RDD分区对应N个父RDD分区（如co-paritioned（协同划分）过的Join）。 宽依赖是指子RDD的分区依赖于父RDD的多个分区或所有分区，即存在一个父RDD的一个分区对应一个子RDD的多个分区。 1个父RDD分区对应多个子RDD分区，这其中又分两种情况： 1个父RDD对应所有子RDD分区（未经协同划分的Join）或者1个父RDD对应非全部的多个RDD分区（如groupByKey）。 本质理解：根据父RDD分区是对应1个还是多个子RDD分区来区分窄依赖（父分区对应一个子分区）和宽依赖（父分区对应多个子分 区）。如果对应多个，则当容错重算分区时，因为父分区数据只有一部分是需要重算子分区的，其余数据重算就造成了冗余计算。 对于宽依赖，Stage计算的输入和输出在不同的节点上，对于输入节点完好，而输出节点死机的情况，通过重新计算恢复数据这种情况下，这种方法容错是有效的，否则无效，因为无法重试，需要向上追溯其祖先看是否可以重试（这就是lineage，血统的意思），窄依赖对于数据的重算开销要远小于宽依赖的数据重算开销。 窄依赖和宽依赖的概念主要用在两个地方：一个是容错中相当于Redo日志的功能；另一个是在调度中构建DAG作为不同Stage的划分点。 依赖关系的特性 第一，窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据；宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。第二，数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复；对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。 容错原理 在容错机制中，如果一个节点死机了，而且运算窄依赖，则只要把丢失的父RDD分区重算即可，不依赖于其他节点。而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。可以这样理解开销的经济与否：在窄依赖中，在子RDD的分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算。在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区用的，会有一部分数据相当于对应的是未丢失的子RDD分区中需要的数据，这样就会产生冗余计算开销，这也是宽依赖开销更大的原因。因此如果使用Checkpoint算子来做检查点，不仅要考虑Lineage是否足够长，也要考虑是否有宽依赖，对宽依赖加Checkpoint是最物有所值的。 Checkpoint机制通过上述分析可以看出在以下两种情况下，RDD需要加检查点。 DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。 在宽依赖上做Checkpoint获得的收益更大。 由于RDD是只读的，所以Spark的RDD计算中一致性不是主要关心的内容，内存相对容易管理，这也是设计者很有远见的地方，这样减少了框架的复杂性，提升了性能和可扩展性，为以后上层框架的丰富奠定了强有力的基础。 在RDD计算中，通过检查点机制进行容错，传统做检查点有两种方式：通过冗余数据和日志记录更新操作。在RDD中的doCheckPoint方法相当于通过冗余数据来缓存数据，而之前介绍的血统就是通过相当粗粒度的记录更新操作来实现容错的。 检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。 一个 Streaming Application 往往需要7*24不间断的跑，所以需要有抵御意外的能力（比如机器或者系统挂掉，JVM crash等）。为了让这成为可能，Spark Streaming需要 checkpoint 足够多信息至一个具有容错设计的存储系统才能让 Application 从失败中恢复。Spark Streaming 会 checkpoint 两种类型的数据。 Metadata（元数据） checkpointing - 保存定义了 Streaming 计算逻辑至类似 HDFS 的支持容错的存储系统。用来恢复 driver，元数据包括： 配置 - 用于创建该 streaming application 的所有配置 DStream 操作 - DStream 一些列的操作 未完成的 batches - 那些提交了 job 但尚未执行或未完成的 batches Data checkpointing - 保存已生成的RDDs至可靠的存储。这在某些 stateful 转换中是需要的，在这种转换中，生成 RDD 需要依赖前面的 batches，会导致依赖链随着时间而变长。为了避免这种没有尽头的变长，要定期将中间生成的 RDDs 保存到可靠存储来切断依赖链 具体来说，metadata checkpointing主要还是从drvier失败中恢复，而Data Checkpoing用于对有状态的transformation操作进行checkpointing Checkpointing具体的使用方式时通过下列方法： 什么时候需要启用 checkpoint？ 什么时候该启用 checkpoint 呢？满足以下任一条件： 使用了 stateful 转换 - 如果 application 中使用了updateStateByKey或reduceByKeyAndWindow等 stateful 操作，必须提供 checkpoint 目录来允许定时的 RDD checkpoint 希望能从意外中恢复 driver 如果 streaming app 没有 stateful 操作，也允许 driver 挂掉后再次重启的进度丢失，就没有启用 checkpoint的必要了。 如何使用 checkpoint？ 启用 checkpoint，需要设置一个支持容错 的、可靠的文件系统（如 HDFS、s3 等）目录来保存 checkpoint 数据。通过调用 streamingContext.checkpoint(checkpointDirectory) 来完成。另外，如果你想让你的 application 能从 driver 失败中恢复，你的 application 要满足： 若 application 为首次重启，将创建一个新的 StreamContext 实例 如果 application 是从失败中重启，将会从 checkpoint 目录导入 checkpoint 数据来重新创建 StreamingContext 实例 通过 StreamingContext.getOrCreate 可以达到目的： 如果 checkpointDirectory 存在，那么 context 将导入 checkpoint 数据。如果目录不存在，函数 functionToCreateContext 将被调用并创建新的 context 除调用 getOrCreate 外，还需要你的集群模式支持 driver 挂掉之后重启之。例如，在 yarn 模式下，driver 是运行在 ApplicationMaster 中，若 ApplicationMaster 挂掉，yarn 会自动在另一个节点上启动一个新的 ApplicationMaster。 需要注意的是，随着 streaming application 的持续运行，checkpoint 数据占用的存储空间会不断变大。因此，需要小心设置checkpoint 的时间间隔。设置得越小，checkpoint 次数会越多，占用空间会越大；如果设置越大，会导致恢复时丢失的数据和进度越多。一般推荐设置为 batch duration 的5~10倍。 导出 checkpoint 数据 上文提到，checkpoint 数据会定时导出到可靠的存储系统，那么 在什么时机进行 checkpoint checkpoint 的形式是怎么样的 checkpoint 的时机 在 Spark Streaming 中，JobGenerator 用于生成每个 batch 对应的 jobs，它有一个定时器，定时器的周期即初始化 StreamingContext 时设置的 batchDuration。这个周期一到，JobGenerator 将调用generateJobs方法来生成并提交 jobs，这之后调用 doCheckpoint 方法来进行 checkpoint。doCheckpoint 方法中，会判断当前时间与 streaming application start 的时间之差是否是 checkpoint duration 的倍数，只有在是的情况下才进行 checkpoint。 checkpoint 的形式 最终 checkpoint 的形式是将类 Checkpoint的实例序列化后写入外部存储，值得一提的是，有专门的一条线程来做将序列化后的 checkpoint 写入外部存储。类 Checkpoint 包含以下数据： 除了 Checkpoint 类，还有 CheckpointWriter 类用来导出 checkpoint，CheckpointReader 用来导入 checkpoint Checkpoint 的局限 Spark Streaming 的 checkpoint 机制看起来很美好，却有一个硬伤。上文提到最终刷到外部存储的是类 Checkpoint 对象序列化后的数据。那么在 Spark Streaming application 重新编译后，再去反序列化 checkpoint 数据就会失败。这个时候就必须新建 StreamingContext。 针对这种情况，在我们结合 Spark Streaming + kafka 的应用中，我们自行维护了消费的 offsets，这样一来及时重新编译 application，还是可以从需要的 offsets 来消费数据，这里只是举个例子，不详细展开了。 为什么要用Yarn来部署Spark?因为 Yarn 支持动态资源配置。Standalone 模式只支持简单的固定资源分配策略，每个任务固定数量的 core，各 Job 按顺序依次分配在资源，资源不够的时候就排队。这种模式比较适合单用户的情况，多用户的情境下，会有可能有些用户的任务得不到资源。 Yarn 作为通用的种子资源调度平台，除了 Spark 提供调度服务之外，还可以为其他系统提供调度，如 Hadoop MapReduce, Hive 等。 说说yarn-cluster和yarn-client的异同点。 cluster 模式会在集群的某个节点上为 Spark 程序启动一个称为 Master 的进程，然后 Driver 程序会运行正在这个 Master 进程内部，由这种进程来启动 Driver 程序，客户端完成提交的步骤后就可以退出，不需要等待 Spark 程序运行结束，这是四一职中适合生产环境的运行方式 client 模式也有一个 Master 进程，但是 Driver 程序不会运行在这个 Master 进程内部，而是运行在本地，只是通过 Master 来申请资源，直到运行结束，这种模式非常适合需要交互的计算。 显然 Driver 在 client 模式下会对本地资源造成一定的压力。 解释一下 groupByKey, reduceByKey 还有 reduceByKeyLocallygroupByKey 该函数用于将RDD[K,V]中每个K对应的V值，合并到一个集合Iterable[V]中， 参数numPartitions用于指定分区数； 参数partitioner用于指定分区函数； reduceByKey 该函数用于将RDD[K,V]中每个K对应的V值根据映射函数来运算。 参数numPartitions用于指定分区数； 参数partitioner用于指定分区函数； reduceByKeyLocally 该函数将RDD[K,V]中每个K对应的V值根据映射函数来运算，运算结果映射到一个Map[K,V]中，而不是RDD[K,V]。 groupByKey和reduceByKey是属于Transformation还是 Action？前者，因为 Action 输出的不再是 RDD 了，也就意味着输出不是分布式的，而是回送到 Driver 程序。以上两种操作都是返回 RDD，所以应该属于 Transformation。 说说 persist() 和 cache() 的异同从源码上分析，参考： 可以解释一下这两段程序的异同吗 所有在 Driver 程序追踪的代码看上去好像在 Driver 上计算，实际上都不在本地，每个 RDD 操作都被转换成 Job 分发至集群的执行器 Executor 进程中运行，即便是单机本地运行模式，也是在单独的执行器进程上运行，与 Driver 进程属于不用的进程。所以每个 Job 的执行，都会经历序列化、网络传输、反序列化和运行的过程。 再具体一点解释是 foreach 中的匿名函数 x =&gt; counter += x 首先会被序列化然后被传入计算节点，反序列化之后再运行，因为 foreach 是 Action 操作，结果会返回到 Driver 进程中。 在序列化的时候，Spark 会将 Job 运行所依赖的变量、方法全部打包在一起序列化，相当于它们的副本，所以 counter 会一起被序列化，然后传输到计算节点，是计算节点上的 counter 会自增，而 Driver 程序追踪的 counter 则不会发生变化。执行完成之后，结果会返回到 Driver 程序中。而 Driver 中的 counter 依然是当初的那个 Driver 的值为0。 因此说，RDD 操作不能嵌套调用，即在 RDD 操作传入的函数参数的函数体中，不可以出现 RDD 调用。 说说map和mapPartitions的区别map 中的 func 作用的是 RDD 中每一个元素，而 mapPartitioons 中的 func 作用的对象是 RDD 的一整个分区。所以 func 的类型是 Iterator&lt;T&gt; =&gt; Iterator&lt;T&gt;，其中 T 是输入 RDD 的元素类型。 说说Spark支持的3种集群管理器Standalone 模式: 资源管理器是 Master 节点，调度策略相对单一，只支持先进先出模式。 Hadoop Yarn 模式: 资源管理器是 Yarn 集群，主要用来管理资源。Yarn 支持动态资源的管理，还可以调度其他实现了 Yarn 调度接口的集群计算，非常适用于多个集群同时部署的场景，是目前最流行的一种资源管理系统。 Apache Mesos: Mesos 是专门用于分布式系统资源管理的开源系统，与 Yarn 一样是 C++ 开发，可以对集群中的资源做弹性管理。 说说Worker和Excutor的异同Worker 是指每个及节点上启动的一个进程，负责管理本节点，jps 可以看到 Worker 进程在运行。Excutor 每个Spark 程序在每个节点上启动的一个进程，专属于一个 Spark 程序，与 Spark 程序有相同的生命周期，负责 Spark 在节点上启动的 Task，管理内存和磁盘。如果一个节点上有多个 Spark 程序，那么相应就会启动多个执行器。 说说Spark提供的两种共享变量Spark 程序的大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算，这些函数在不同的节点上并发执行，内部的变量有不同的作用域，不能相互访问，有些情况下不太方便。 广播变量，是一个只读对象，在所有节点上都有一份缓存，创建方法是 SparkContext.broadcast()。 创建之后再更新它的值是没有意义的，一般用 val 来修改定义。 计数器，只能增加，可以用计数或求和，支持自定义类型。 创建方法是 SparkContext.accumulator(V, name)。 只有 Driver 程序可以读这个计算器的变量，RDD 操作中读取计数器变量是无意义的。 以上两种类型都是 Spark 的共享变量。 说说Spark的高可用和容错 解释一下Spark Master的选举过程参考： 说说Spark如何实现序列化组件的Spark通过两种方式来创建序列化器 Java序列化在默认情况下，Spark采用Java的ObjectOutputStream序列化一个对象。该方式适用于所有实现了java.io.Serializable的类。通过继承java.io.Externalizable，你能进一步控制序列化的性能。Java序列化非常灵活，但是速度较慢，在某些情况下序列化的结果也比较大。 Kryo序列化Spark也能使用Kryo（版本2）序列化对象。Kryo不但速度极快，而且产生的结果更为紧凑（通常能提高10倍）。Kryo的缺点是不支持所有类型，为了更好的性能，你需要提前注册程序中所使用的类（class）。 Java的序列化比较简单，就和前面的一样，下面主要介绍Kryo序列化的使用。 Kryo序列化怎么用？可以在创建SparkContext之前，通过调用System.setProperty(“spark.serializer”, “spark.KryoSerializer”)，将序列化方式切换成Kryo。 但是Kryo需要用户进行注册，这也是为什么Kryo不能成为Spark序列化默认方式的唯一原因，但是建议对于任何“网络密集型”（network-intensive)的应用，都采用这种方式进行序列化方式。 Kryo文档描述了很多便于注册的高级选项，例如添加用户自定义的序列化代码。 如果对象非常大，你还需要增加属性spark.kryoserializer.buffer.mb的值。该属性的默认值是32，但是该属性需要足够大以便能够容纳需要序列化的最大对象。 最后，如果你不注册你的类，Kryo仍然可以工作，但是需要为了每一个对象保存其对应的全类名（full class name),这是非常浪费的。 说说对Master的理解Master 是 local-cluster 部署模式和 Standalone 部署模式中，整个 Spark 集群最为重要的组件之一，分担了对整个集群资源的管理和分配的工作。 local-cluser 下，Master 作为 JVM 进程的对象启动，而在 Standalone 模式下，就是单独的进程启动。 说说什么是窗口间隔和滑动间隔也叫 WriteAheadLogs，通常被用于数据库和文件系统中，保证数据操作的持久性。预写日志通常是先将操作写入到一个持久可靠的日志文件中，然后才对数据施加该操作，当加入施加该操作中出现异常，可以通过读取日志文件并重新施加该操作，从而恢复系统。 当 WAL 开启后，所有收到的数据同时保存到了容错文件系统的日志文件中，当 Spark Streaming 失败，这些接受到的数据也不会丢失。另外，接收数据的正确性只在数据被预写到日志以后接收器才会确认。已经缓存但还没有保存的数据可以在 Driver 重新启动之后由数据源再发送一次（经常问）。 这两个机制保证了数据的零丢失，即所有的数据要么从日志中恢复，要么由数据源重发。 Spark Streaming小文件问题使用 Spark Streaming 时，如果实时计算结果要写入到 HDFS，那么不可避免的会遇到一个问题，那就是在默认情况下会产生非常多的小文件，这是由 Spark Streaming 的微批处理模式和 DStream(RDD) 的分布式(partition)特性导致的，Spark Streaming 为每个 Partition 启动一个独立的线程（一个 task/partition 一个线程）来处理数据，一旦文件输出到 HDFS，那么这个文件流就关闭了，再来一个 batch 的 parttition 任务，就再使用一个新的文件流，那么假设，一个 batch 为10s，每个输出的 DStream 有32个 partition，那么一个小时产生的文件数将会达到(3600/10)*32=11520个之多。众多小文件带来的结果是有大量的文件元信息，比如文件的 location、文件大小、block number 等需要 NameNode 来维护，NameNode 会因此鸭梨山大。不管是什么格式的文件，parquet、text、JSON 或者 Avro，都会遇到这种小文件问题，这里讨论几种处理 Spark Streaming 小文件的典型方法。 增加 batch 大小: 这种方法很容易理解，batch 越大，从外部接收的 event 就越多，内存积累的数据也就越多，那么输出的文件数也就会变少，比如上边的时间从10s增加为100s，那么一个小时的文件数量就会减少到1152个。 但别高兴太早，实时业务能等那么久吗，本来人家10s看到结果更新一次，现在要等快两分钟，是人都会骂娘。 所以这种方法适用的场景是消息实时到达，但不想挤压在一起处理，因为挤压在一起处理的话，批处理任务在干等，这时就可以采用这种方法。 Coalesce大法好: 文章开头讲了，小文件的基数是 batch_number * partition_number，而第一种方法是减少 batch_number，那么这种方法就是减少 partition_number 了，这个 api 不细说，就是减少初始的分区个数。 看过 spark 源码的童鞋都知道，对于窄依赖，一个子 RDD 的 partition 规则继承父 RDD，对于宽依赖(就是那些个叉叉叉ByKey操作)，如果没有特殊指定分区个数，也继承自父 rdd。 那么初始的 SourceDstream 是几个 partiion，最终的输出就是几个 partition。 所以 Coalesce 大法的好处就是，可以在最终要输出的时候，来减少一把 partition 个数。 但是这个方法的缺点也很明显，本来是32个线程在写256M数据，现在可能变成了4个线程在写256M数据，而没有写完成这256M数据，这个 batch 是不算结束的。 那么一个 batch 的处理时延必定增长，batch 挤压会逐渐增大。 Spark Streaming 外部来处理: 我们既然把数据输出到 hdfs，那么说明肯定是要用 Hive 或者 Spark Sql 这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天，这样来分区的(注意不要和 Spark Streaming 的分区混淆，这里的分区，是用来做分区裁剪优化的)，那么我们可以考虑在 Spark Streaming 外再启动定时的批处理任务来合并 Spark Streaming 产生的小文件。 这种方法不是很直接，但是却比较有用，“性价比”较高，唯一要注意的是，批处理的合并任务在时间切割上要把握好，搞不好就可能会去合并一个还在写入的 Spark Streaming 小文件。 自己调用 foreach 去 append: Spark Streaming 提供的 foreach 这个 outout 类 api （一种 Action 操作），可以让我们自定义输出计算结果的方法。 那么我们其实也可以利用这个特性，那就是每个 batch 在要写文件时，并不是去生成一个新的文件流，而是把之前的文件打开。 考虑这种方法的可行性，首先，HDFS 上的文件不支持修改，但是很多都支持追加，那么每个 batch 的每个 partition 就对应一个输出文件，每次都去追加这个 partition 对应的输出文件，这样也可以实现减少文件数量的目的。 这种方法要注意的就是不能无限制的追加，当判断一个文件已经达到某一个阈值时，就要产生一个新的文件进行追加了。 所以大概就是一直32个文件。 参考资料引用自独孤九剑-Spark面试80连击(上)"},{"title":"Spark Join","date":"2021-07-06T12:10:20.000Z","url":"/2021/07/06/Spark-Join/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"],["Spark SQL","/tags/Spark-SQL/"]],"categories":[[" ",""]],"content":"Join 是 SQL 中非常重要的语法，只要稍微复杂一点的场景就离不开 Join，甚至是多表 Join，如下图，Join 有三个关注点：Join 方式、Join 条件、过滤条件 图片引用自Spark SQL 之 Join 实现 Spark Join 实现方式在聊实现方式前，需要了解什么 Hash Join，hash join 是指将一个表根据 Join Key 做 hash（暂且称为查找表），然后遍历另一个表（暂且称为遍历表），遍历时在 hash 表中根据 join key 查找，这样时间复杂度就是 O(M(遍历一个表) + 1(hash 查找))，入下图，Build Table 为查找表，Probe Table 为遍历表 图片引用自Spark难点 | Join的实现原理 Broadcast Hash Joinok，对 hash join 了解后，再来聊聊 Broadcast Hash Join。在 join 过程中有这么一种情况——一张很大的表 join 一张很小的表（一般是维表），这时如果对大表做 shuffle 代价可能比较大，Spark 会对这种情况进行优化，将小表作为广播变量广播到各个节点中，然后在节点中做 hash join。这样对一个大表的 shuffle 就变成对一个小表的广播，而 Spark 广播变量使用 BitTorrent 进行优化(感兴趣可以点这Spark Accumulator &amp; Broadcast)，性能比 Shuffle 有很大的提省，只要每个节点的大表分区做一个 map 就可以完成，这种方式也被称为 map join。 图片引用自Spark Join Strategies — How & What? 虽然 Broadcast Hash Join 的方式快，但是随着广播的表大小增长，性能也逐渐下降，所以 Spark 只会将小于 spark.sql.autoBroadcastJoinThreshold (默认 10M)的表采用 Broadcast Hash Join，spark.sql.autoBroadcastJoinThreshold 参数设置为 -1，可以关闭 BHJ，实现可见 org.apache.spark.sql.execution.joins.BroadcastHashJoinExec 特点： 只能用于等值连接 除了 full outer join，支持所有 join 广播表大小 &lt; spark.sql.autoBroadcastJoinThreshold (default 10M) Shuffle Hash Join当 Broadcast Table 大到一定程度后，将整个表广播已经不太划算了，还不如 shuffle。这就会用到 Shuffle Hash Join，Spark 会根据 Join Key 进行 shuffle，那么相同的 key 一定都在同一个节点中，再根据 Hash Join 的方式进行单机 Join 图片引用自Spark Join Strategies — How & What? Shuffle Hash Join 需要在内存中建立 hash table 所以有 OOM 风险，使用 Shuffle Hash Join 的前提为 spark.sql.join.preferSortMergeJoin 必须为 false，实现见 org.apache.spark.sql.execution.joins.ShuffledHashJoinExec 特点： 只支持等值连接 除了 full outer join，支持所有 join spark.sql.join.preferSortMergeJoin 必须为 false 小表的大小 &lt; spark.sql.autoBroadcastJoinThreshold(default 10M) * spark.sql.shuffle.partitions(default 200) 小表的大小 * 3 &lt;= 大表大小 Sort Merge Join既然 Shuffle 不可避免，那么有没有其他优化的方式呢？Spark Shuffle 对排序有着很好的支持，所以在 Shuffle Write完之后，两个表都是局部有序的，那么可不可以在 Shuffle Read 阶段就完成 Join。由于两个表根据 Join Key 分区数据都是有序的，那么在 Shuffle Read 时，可以根据采用 Hash Join 的思想，只不过这次的查找表不是 hash 查找，而是顺序查找。对遍历表一条一条遍历，对查找表顺序查找，下一条数据只要从当前位置查找即可，不需要从头开始查找。当 Shuffle 完成时，Join 也就完成了。 图片引用自Spark Join Strategies — How & What? 显然，如果要使用 Sort Merge Join ，Join Key 必须是可排序的，实现可见 org.apache.spark.sql.execution.joins.SortMergeJoinExec 特点： 只支持等值连接 支持所有类型的 join Join key 必须可排序 Broadcast nested loop join 如上面代码所示，Broadcast nested loop join 通过循环嵌套的方式进行 join，效率非常低。具体实现见 org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoinExec 特点： 支持等值和不等值连接 支持所有类型的 join Cartesian product join(Shuffle-and-replicate nested loop join)笛卡尔积 join，与普通 SQL 一样，如果 join 时不加连接条件就会产生笛卡尔积连接。具体实现可以看看 org.apache.spark.sql.execution.joins.CartesianProductExec 特点： 支持等值和不等值连接 只支持 inner join 需要开启 spark.conf.set(&quot;spark.sql.crossJoin.enabled&quot;, &quot;true&quot;) Join 方式选择既然有这么多种 Join 方式，那么 Spark 是怎么选择合适的 Join 方式呢？ Spark 根据等值和非等值连接进行划分 等值连接 其中用户选择遵循下方代码顺序 即 Broadcast Hash Join, Sort Merge Join, Shuffle Hash Join, Cartesian Product Join, Broadcast Nested Loop Join 的顺序 非等值连接 用户选择顺序为： 即 Broadcast Nested Loop Join, Cartesian Product Join。上图有两次 Broadcast Nested Loop Join，是因为第一次 Broadcast Nested Loop Join 会先尝试是否能广播左表或者右表，如果都不能则选择 Cartesian Product Join，最后再用 Broadcast Nested Loop Join 兜底 Spark 中特殊的 Joinleft semi joinleft semi join 是以左表为准，如果查找成功就返回左表的记录，如果查找失败则返回 null，如下图所示。 图片引用自Spark SQL 之 Join 实现 left anti joinleft anti join 则是与 left semi join 相反，也是以左表为准，如果查找成功就返回 null，如果查找失败则返回左表记录，如下图 图片引用自Spark SQL 之 Join 实现 我不知道这两种特殊的 Join 方式是不是 Spark 特有的，但是是我学习 Spark 之后才知道有这两种 Join 方式 参考资料Spark Join Strategies — How &amp; What? 每个 Spark 工程师都应该知道的五种 Join 策略 Spark SQL 之 Join 实现"},{"title":"Spark Accumulator & Broadcast","date":"2021-06-24T04:29:44.000Z","url":"/2021/06/24/Spark-Accumulator-Broadcast/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"Spark 中有两类共享变量，即累加器（Accumulator）和广播变量（Broadcast） Accumulator累加器，顾名思义，是用于累加计数的，那么为什么需要累加器呢，普通变量不能计数吗？先看一段代码 这段代码的输出结果为 0，为什么呢？这里涉及到一个 Spark 闭包问题，所谓的闭包可以理解为 在函数作用域中修改作用域外的变量。在计算前 Spark 会将闭包序列化并发送到 executor 上，上面代码中会将 counter 序列化打包，然后在 executor 中反序列化，这时 Driver 端和每个 Executor 中都有一个 counter，但是 Executor 中的 counter Driver 端的副本，也就是说如果在 Executor 中修改了 counter 是不会影响 Driver 端 counter 的值，所以没法起到计数的效果。所以计数就需要用到 Accumulator 从 LongAccumulator 代码看来 _sum 也只是普通的变量，那为什么 LongAccumulator 中的变量就能起到计数的效果呢在 LongAccumulator 里有一个 merge 方法，这个 merge 方法在 task 运行完成后被调用，会将不同 Executor 的变量合并到 Driver 上。 需要注意的是 Accumulator 的值只能在 Driver 端查看。 上图左边为使用普通变量，右图为使用 Spark Accumulator，区别就在于 Spark Accumulator 在 Task 任务执行后将变量回传给 Driver 进行累加。 Accumulator 使用Spark 内置有三种 Accumulator，分别是 LongAccumulator、DoubleAccumulator 和 CollectionAccumulator，分别对应 Long、Double、List 三种计数类型。在使用 Accumulator 时需要 action 算子触发计算。 SparkContext 集成了三种 Accumulator，在使用时只需要传入一个 Accumulator name 即可，在 Spark Web UI 中的 Stages 页面可以查看声明的 Accumulator 自定义 Accumulator自定义 Accumulatro 只需要继承 AccumulatorV2 即可 完成定义后，在使用时将 Accumulator 在 SparkContext 中注册即可 运行上面的代码发现每次运行的结果都不一样 这是因为 merge 的时候顺序是不确定的，哪个task 先执行完就先 merge 哪个，所以在自定义 Accumulator 的时候一定要注意合并顺序不能影响最终结果，这样才算是正确的 Accumulator Accumulator 注意事项下面这段代码，counter 第一次打印 15，第二次打印 30 因为调用了两次 action 算子，这段代码会被构建成两个 job，每个 job 是从 DAG 最开始进行计算，都会对 counter 进行操作，所以第二次会打印 30 我们只需要将中间结果 cache 隔断之前的血缘即可 ，就能解决这个问题 这样两次打印的值就一样了。 Accumulator 小技巧Accumulator 不仅可以作为累加器，还能作为“累减器“，只需要累加的数值改成负值即可，只适用于数值计算 BroadcastSpark 中另一个共享变量就是广播变量（broadcast）。一般一个 Executor 中会有多个 Task，如果不使用广播变量，Spark 需要将变量副本传输到每个 Task 中，造成带宽、内存和磁盘的浪费，降低执行效率，使用广播变量则只会传输一份副本到 Executor 中，Executor 中的 Task 共享这一份副本。假设有 100 个 Executor，每个 Executor 中有 10 个 Task，然后有一个 10M 大小的 List 需要考拷贝副本，如果不使用广播变量的情况需要拷贝 100 * 10 * 10 = 10000M，大概 10G；如果使用广播变量只需要 100 * 10 = 1000M，减小了 10 倍。所以广播变量是优化 Spark 任务的一个小技巧。 Broadcast 原理broadcast 传输文件采用 BitTorrent 协议，也就是常说的 BT 下载，和点对点（point-to-point）的协议程序不同，它是用户群对用户群（peer-to-peer），可以看下这个小游戏 BitTorrent，大概思想为把一个文件切分成多个块，每个下载的机器上都存放一块或多块，有新机器加入下载就可以到不同的机器上下载不同的块，降低 Server 的负载。 Spark Broadcast 的原理与 BitTorrent 类似，Spark 使用 blockManager 管理文件 block 写流程 将需要广播的变量切分成一个个 chunk (默认 4M） 计算这些 chunk 的 Adler-32 checksum 每个 chunk 加上一个 pieceId （ “broadcast_” + broadcastId + “_” + “piece” + number），这个 pieceId 作为 blockId 写入文件系统 读流程 随机获取 pieceId 根据 blockManager 获取 block 校验校验和 按照 pieceId 顺序拼接各个 block Broadcast 使用 Broadcast 只读问题为什么 Broadcast 只能读不能修改？因为广播变量会传输到多个 Executor 上，如果某个 Executor 能够修改了 Broadcast，那么 Spark 就要保证修改的 Broadcast 能及时同步到每台机器上，在同步的时候要保证数据的一致性以及同步失败怎么容错等等问题，所以 Spark 干脆就让 Broadcast 不可修改 参考资料Spark笔记之累加器（Accumulator） spark 广播变量的设计和实现"},{"title":"Spark repartition vs coalesce","date":"2021-06-15T10:00:00.000Z","url":"/2021/06/15/Spark%20repartition%20vs%20coalesce/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"在平时经常需要对 RDD 重新分区，或增加分区以提高并行度，或减少分区写文件以减少小文件数量，对 RDD 的分区数量操作有 repartition 和 coalesce 两个算子 repartitionrepartition 算子会返回一个指定分区数的新 RDD，可以用来增加或者减小分区数量以调整 RDD 的并行度。不过这种增加和减小分区数量是以 shuffle 为代价换来的。 repartition 的代码很简单，接受一个分区数参数，然后调用 coalesce 方法 coalescecoalesce 有两种模式，分别是 shuffle 和非 shuffle 模式，从 repartition 调用的是 shuffle 模式，这一点从参数可以看出来。 如果 shuffle 参数为 true 则会将一个 ShuffledRDD 封装进 CoalescedRDD。如果 shuffle 参数为 false(默认) 则创建一个 CoalescedRDD 对象。主要看看非 shuffle 模式 coalesce 的非 shuffle 模式只能用来减少分区，例如有 1000 个分区，可用用 coalesce(100) 减少至 100 个分区，并且不会 shuffle；如果传入的参数比现在的分区数量多，则不会有任何效果，如果要添加分区数量可以使用 repartition 或者使用 coalesce 时 shuffle 参数传入 false。 原理主要原理就是将多个 partition 划分成一个个 partitionGroup，例如前面的例子，有 1000 个分区，需要减少至 100 个分区，那么就会创建 100 个 partitionGroup，每个 partitionGroup 都有 10 个 partition，相当于将 1000 个分区分成 100 组，每组有 10 个分区，而一个 partitionGroup 则作为 CoalescedRDD 的一个分区。 重点看下 getPartitions 和 compute 方法 这里会使用 DefaultPartitionCoalescer 进行 coalesce，然后封装到 CoalescedRDDPartition 中，这样一个 partitionGroup 就封装成一个 partition 了 再看下 DefaultPartitionCoalescer 的 coalesce 通过 setupGroups 和 throwBalls 两个方法之后，会将 dependencesRDD 尽可能按 preferredLocation 划分好分组，放入 val groupArr = ArrayBuffer[PartitionGroup]() 中，最后调用 DefaultPartitionCoalescer 的 getPartitions 返回 PartitionsGroup 数组 再来看下 compute 方法 compute 中的 Partition 就是一个 PartitionGroup，compute 迭代一个 partition 就是迭代一个 partitionGroup 也就是上游的一组 partition，以此来达到减少分区的作用 总结repartition 算子既可以增加分区数量也可以减少分区数量，但代价是会造成 shuffle，所以如果是减少分区操作可以使用 coalesce 算子。使用 coalesce 算子时，如果 shuffle 参数为 false(默认) 则只能减少分区数量，如果 shuffle 参数为 true 则可以增加或减少分数数，相当于 repartition 算子。 参考文章浪尖说spark的coalesce的利弊及原理 Spark RDD之Partition"},{"title":"Spark 聚合算子","date":"2021-06-14T07:44:09.000Z","url":"/2021/06/14/Spark%20%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"]],"categories":[[" ",""]],"content":"combineByKey在理解 groupByKey 和 reduceByKey 之前先看看 combineByKey combineBykey 需要三个参数：createCombiner、mergeValue、mergeCombiners createCombiner：当碰到未出现过 key 时需要做的处理 mergeValue：当碰到出现过的 key 时需要做的处理 mergeCombiners：分布式环境下，多台机器的结果如何合并 假设我们有下列一个 kv list，现在需要将 value 按照类别放到不同的 list 中，将 key 为水果的 value 都放到一个单独 list 中，将 key 为动物的 value 放到一个单独的 list 中 调用完 combineByKey 后，会返回一个 (String, List[String]) 的映射关系，我们把 res 打印一下可以得出 再贴一段代码，这段代码只需记住 mapSideCombine 默认值为 true 即可 groupByKey再看看 groupByKey，groupByKey 有一个同胞兄弟 groupBy，groupBy 最终还是调用的 groupByKey，看看 groupByKey 的代码 可以看出 groupBy 也是调用 combineByKeyWithClassTag，只不过 createCombiner、mergeValue、mergeCombiners 不是自定义的。 groupByKey 的 createCombiner 是创建一个 buffer，这个 buffer 采用数组存储 mergeValue 则是将值加入数组 mergeCombiners 就将两个数组合并 注意 groupByKey 调用 combineByKeyWithClassTag 时传入的 mapSideCombine 为 false 表示不进行预聚合，即 mergeValue 不是发生在 shuffle 前 图片来自： reduceByKey再看看 reduceByKey 的实现，reduceByKey 也有一个兄弟叫 reduce，不过 reduce 没有调用 reduceByKey，而是使用的 reduceLeft。直接上 reduceByKey 的代码 reduceByKey 最终也是调用 combineByKeyWithClassTag，只不过 mergeValue 和 mergeCombiners 都使用我们自定义的一个函数，不过需要注意这个函数 func: (V, V) =&gt; V，这个函数要求输入的两个参数类型必须一致且返回的类型和输入类型一致，所以一般将输入的两个参数进行聚合再返回。所以聚合后我们得到的是 k -&gt; v 。所以在分布式环境下进行 mergeCombiners 时可以使用和 mergeValue 同一个函数即可。 图片来自： 从上面图中可以看出，groupByKey 在 shuffle 前是不进行预聚合的，而 reduceBykey 则是将 key 相同的数据聚合成一个，图中只是最简单的场景，生产环境中场景更加复杂，数据量大得多，所以在数据量相同的情况，groupByKey 的性能必然要比 reduceByKey 更差。 aggregateByKey先看代码 aggregateByKey 也有三个参数： zeroValue ：‘零值’，聚合之前的初始值，必须是可变的，每一个分区中的每一个 key 都有一个初始值 seqOp：将 V 聚合进 U 的操作，作用于同一个分区里 combOp：将两个 U 合并的操作，作用于不同分区之间 aggregateByKey 和 combineByKey 非常相似，重点关注 combineByKeyWithClassTag[U]((v: V) =&gt; cleanedSeqOp(createZero(), v), cleanedSeqOp, combOp, partitioner) 发现 createCombiner 和 mergeValue 使用的是统一个函数，只不过 createCombiner 时传入了零值。所以当 createCombiner 和 mergeValue 操作一样时优先使用 aggregateByKey foldByKey show code 这个 foldByKey 相较于 aggregateByKey combine 的三个参数都使用一个函数，即 crateCombiner、mergeValue、mergeCombiners 都相同，只不过 crateCombiner 使用了零值 进阶版 combineByKey 这两个进阶版的 combineByKey 比普通版的 combineByKey 分别多了可以指定分区数的 numPartitions 参数和指定分区器的 Partitioner 参数，其它都一致。如果不指定分区器，则使用 self 即调用者的分区器类型 groupByKey、reduceByKey 和 aggregateByKey 的进阶版也一样，都只是增加了指定分区数和分区器的参数。 countByKeycountByKey 顾名思义，能根据 key 计数，看看源码 countByKey 使用的就是 reduceByKey countApproxDistinctByKey统计每个 key 下去重后的近似值，可以根据 relativeSD 参数设置相对精度，默认值是 0.05，但必须大于 0.000017。countApproxDistinctByKey 底层采用 HyperLogLog 统计 总结 combineByKey、groupByKey、reduceBykey、aggregateByKey、foldByKey 这几个算子都是基于 combineByKeyWithClassTag，而 combineByKeyWithClassTag 的核心参数就三个，分别是 crateCombiner、mergeValue、mergeCombiners。 五个算子的主要区别：combinerByKey 和 aggregateByKey 输入输出类型可以不一致，而 reduceByKey 和 foldByKey 要求输入和输出参数一致，groupByKey 就不是聚合算子，只能算分组算子，且性能最差，因为会 shuffle 所有值。 当 crateCombiner 和 mergeValue 是一样的操作时可以选用 aggregateByKey，而 createCombiner、mergeValue 和 mergeCombiners 都一样时，可以选用 foldByKey，根据需要各取所需即可。 参考文章 结合Spark源码分析, combineByKey, aggregateByKey, foldByKey, reduceByKey Avoid GroupByKey"},{"title":"MySQL 常用命令","date":"2021-04-19T16:00:00.000Z","url":"/2021/04/20/MySQL-command/","tags":[["MySQL","/tags/MySQL/"]],"categories":[[" ",""]],"content":""},{"title":"Spark DataFrame 自增 ID","date":"2021-04-19T04:42:25.000Z","url":"/2021/04/19/Spark-increment-id/","tags":[["Spark","/tags/Spark/"],["DataFrame","/tags/DataFrame/"]],"categories":[[" ",""]],"content":"Spark 添加一列自增 ID 有两种方式，一种是通过 Spark SQL 的窗口函数，另一种是通过 RDD 的 zipWithIndex 算子。 案例： 输出： 窗口函数方式： 输出： zipWithIndex 算子方式： 输出： 上面 .map&#123;case(x, y) =&gt; (x, y + 1)&#125; 的作用是让 id 初始值为 1"},{"title":"Spark on YARN","date":"2021-04-18T08:30:09.000Z","url":"/2021/04/18/Spark-On-YARN/","tags":[["大数据","/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"],["Spark","/tags/Spark/"],["YARN","/tags/YARN/"]],"categories":[[" ",""]],"content":" YARN YARN(Yet Another Resource Negotiator) 是 Hadoop 2.x 新增的资源调度器。首先看下 Hadoop 1.x 是怎么调度的 Hadoop 1.x 的运行流程： client 提交任务给 Job Tracker Job Tracker 接收任务，并根据 Job 的参数向 NameNode 请求包含这些文件的 DataNode 节点列表 Job Tracker 确定执行计划：确认 Map、Reduce 的 Task 数量，将这些 Task 分配到离数据块最近的节点上执行 随着发展部署任务越来越多，虽然 Job Tracker 可以部署多个，但是只有一个 Job Tracker 处于 active 状态，其它的 Job Tracker 都是 standby 并不能接收任务，所以 Job Tracker 变成了 Hadoop 的瓶颈。 那么 YARN 是怎么解决这个问题的呢？YARN 采用 Master/Slave 的结构，采用双层调度架构，第一层是 ResourceManager 和 NodeManager : ResourceManager 是 Master 节点，相当于 Job Tracker，有 Scheduler 和 Application Manager 两个组件，分别用于资源调度和应用管理；NodeManager 是 Slave 节点，可以部署在独立的机器上，用于管理及其上的资源。 第二层是 NodeManager 和 Container ，NodeManager 将 CPU 内存等资源抽象成一个个的 Container 并管理它们的生命周期。 这种架构的好处： Scheduler 由原来管理的 CPU 等资源变成了管理 Container 粒度变粗了，降低了负载。 Application Manager 只需要管理 App Master 不需要管理任务调度的完整信息，也降低了负载。 下图为 YARN 的架构图 组件说明： ResourceManager： 定时调度器(Scheduler)：从本质上来说，定时调度器就是一种策略，或者说一种算法。当 Client 提交一个任务的时候，它会根据所需要的资源以及当前集群的资源状况进行分配。注意，它只负责向应用程序分配资源，并不做监控以及应用程序的状态跟踪。 应用管理器(ApplicationManager)：同样，听名字就能大概知道它是干嘛的。应用管理器就是负责管理 Client 用户提交的应用。上面不是说到定时调度器（Scheduler）不对用户提交的程序监控嘛，其实啊，监控应用的工作正是由应用管理器（ApplicationManager）完成的。 NodeManager: Container：容器（Container）这个东西是 Yarn 对资源做的一层抽象。就像我们平时开发过程中，经常需要对底层一些东西进行封装，只提供给上层一个调用接口一样，Yarn 对资源的管理也是用到了这种思想。 需要注意两点： 容器由 NodeManager 启动和管理，并被它所监控。 容器被 ResourceManager 进行调度。 ApplicationMaster 每当 Client 提交一个 Application 时候，就会新建一个 ApplicationMaster 。由这个 ApplicationMaster 去与 ResourceManager 申请容器资源，获得资源后会将要运行的程序发送到容器上启动，然后进行分布式计算。这个 ApplicationMaster 可能运行在某个 Container 中，也可能运行在 Clinet 中，由不同的部署方式决定。 YARN 的部署流程： Client 向 ResourceManager 提交一个作业。 ResourceManager 向 NodeManager 请求一个 Container 并在这个 Container 中运行 ApplicationMaster ApplicationMaster 向 ResourceManager 注册，注册之后，客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详情以及直接和 ApplicationMaster 交互； ApplicationMaster 启动后将作业拆分一个个的 Task，然后向 ResourceManager 请求 Container 资源用于运行 Task，并定时向 ResourceManager 发送心跳 ApplicationMaster 请求到资源后，ApplicationMaster 会跟对应的 NodeManager 通信，并将 Task 分发到对应的 NodeManager 中的 Container 中运行。 ApplicationMaster 定时向 ResourceManager 发送心跳，汇报作业运行情况。程序运行完成后再向 ResourceManager 注销释放资源。 Spark On YARN 首先看下 Spark 资源管理架构图： 组件说明： ClusterManager ：也就是 Master，是 Spark 的主控节点，可以部署多个 Master，但是只有一个是 active 状态。 Work：是 Spark 的工作节点，向 Master 汇报资源、Executor 的执行状态，由 Master 控制 Worker 的启动。 Driver：是应用程序的驱动程序，每个应用包含多个小任务，Driver 负责推动这些小任务执行。 Executor：是 Spark 的工作进程，由 Worker 管理，负责具体任务的执行。 Spark 的架构与 YARN 的架构非常像，简单看下角色的对比，Master 与 ResourceManager 相对应，Worker 和 NodeManager 对应，Driver 和 ApplicationMaster 对应，Executor 和 Container 相对应。 Spark 的部署模式： Local 模式：部署在同一个进程中，只有 Driver 角色，接受任务后创建 Driver 负责应用的调度执行，不涉及 Master 和 Worker； Local- Cluster 模式：部署在同一个进程上，存在 Master 和 Worker 角色，它们作为独立线程存在于这个进程内； Standalone：Spark 真正的集群模式，在这个模式下 Master 和 Worker 是独立的进程； 第三方部署模式：构建于 YARN 或者 Mesos 上，由第三方负责资源管理。 Spark On YARN-Cluster 部署流程： Client 向 ResourceManager 提交任务 ResourceManager 接收任务并找到一个 Container 创建 ApplicationMaster，此时 ApplicationMaster 上运行的是 Spark Driver ApplicationMaster 向 ResourceManager 申请 Container 并启动 Spark Driver 在 Container 上启动 Spark Executor，并调度 Spark Task 在 Spark Executor 上运行 等作业执行完后向 ResourceManager 注销释放资源 可以看出这个执行流程和 Yarn 对一个任务的处理过程几乎一致，不同的是在 Spark on Yarn 的 Job 处理过程中 App Master、Container 是交由 Spark 相对应的角色去处理的。 Spark on YARN 还有一种部署方式：Spark On YARN-Client，与 Spark On YARN-Cluster 的区别就是 Spark on Yarn-Client 的客户端在提交完任务之后不会将 Spark Driver 托管给 Yarn，而是在客户端运行。App Master 申请完 Container 之后同样也是由 Spark Driver 去启动 Spark Executor，执行任务。 参考资料 深入浅出 Hadoop YARN Spark on Yarn | Spark，从入门到精通 "},{"title":"字符串","date":"2020-09-23T12:31:50.000Z","url":"/2020/09/23/redis-object1/","tags":[["Redis","/tags/Redis/"]],"categories":[[" ",""]],"content":" 字符串字符串的编码有 int、raw、embstr。 整数如果字符串对象保存的是一个整数时，Redis 可以用 long 类型来表示，在上节中 redisObject 的 encoding 置为 REDIS_ENCODING_INT，ptr 将保存整数值（void* 转换为 long）。例如下面这个例子 补充：可以用 long 、double 表示的浮点数在 redis 中也是用字符串值保存 在需要的时候再把字符串转换成浮点值。下表是 redis 保存各种数值类型的编码 数值类型 编码 可以用 long 表示的整数 int 可以用 long 、double 表示的浮点数 embstr、raw 长度太大，无法用 long、double 表示的整数或浮点数 embstr、raw raw 字符串如果字符串对象保存的是一个字符，且字符串长度大于 39 字节，redis 将采用 raw 编码。 embstr 字符串如果保存的字符小于等于 39 字节，redis 采用 embstr 编码保存字符串，embstr 底层实现也是 SDS 只是将 redisObject 与 sdshdr 分配在一块连续的空间，如下图： embstr 优点： raw 编码方式需要调用两次内存分配，分别为 redisObject 、sdshdr 分配空间，而 embstr 只需要调用一次内存分配即可 释放内存时也只需要一次 连续内存能更好的利用缓存 编码转换 int 编码转 raw 当对 int 编码的对象执行一些命令使其不是整数时，将转换成 raw 编码 embstr 编码转 raw redis 没有为 embstr 编码类型编写任何修改程序（只有对 int 和 raw 编码的修改程序），所以 embstr 实际上是只读字符串，任何对 embstr 编码字符串进行修改时，程序总会先转换成 raw 编码。 命令实现 命令 int 编码实现 embstr 编码实现 raw 编码实现 SET 使用 int 编码保存 使用 embstr 编码保存 使用 raw 编码保存 GET 先获取保存的整数，转换成字符串，返回客户端 直接向客户端返回字符串值 直接向客户端返回字符串值 APPEND 先将对象转换成 raw 编码，再按 raw 编码执行操作 先将对象转换成 raw 编码，再按 raw 编码执行操作 调用 sdscatlen 函数，将给定字符串追加到现有字符末尾 INCRBYFLOAT 取出整数值将其转换成 long double 类型的浮点的，再进行加法运算 取出字符串值，尝试转换成浮点数，再进行加法计算，如果不能转换，则返回错误 取出字符串值，尝试转换成浮点数，再进行加法计算，如果不能转换，则返回错误 INCRBY 进行加法计算 embstr 不能执行这个命令，返回错误 raw 不能执行这个命令，返回错误 DECRBY 进行减法计算 embstr 不能执行这个命令，返回错误 raw 不能执行这个命令，返回错误 STRLEN 先将对象转换成 raw 编码，再按 raw 编码执行操作 调用 sdslen 函数 调用 sdslen 函数 SETRANGE 先将对象转换成 raw 编码，再按 raw 编码执行操作 先将对象转换成 raw 编码，再按 raw 编码执行操作 将字符串特定索引上的值设置为给定的字符串 GETRANGE 将对象转换成字符串值，然后取出并返回字符串指定索引上的字符 直接取出并返回字符串指定索引上的字符 直接取出并返回字符串指定索引上的字符 "},{"title":"Redis 对象和编码","date":"2020-09-22T11:29:56.000Z","url":"/2020/09/22/redis-object/","tags":[["Redis","/tags/Redis/"]],"categories":[[" ",""]],"content":" 对象在 Redis 中用对象来表示键和值，每次创建一个键值对时至少创建两个对象，一个用于存放键对象，一个用于存放值对象。例如： 上面命令中，存储键是一个 msg 的字符串对象，存储值是一个 hello redis 的字符串对象。 Redis 的对象由一个 redisObject 结构体表示，结构代码如下： 类型上面代码中的 type 记录对象是什么类型（字符串、集合、哈希、列表、有序集合），type 的常量由下表所示： 常量 对象类型 TYPE 命令输出 REDIS_STRING 字符串 string REDIS_LIST 列表 list REDIS_HASH 哈希 hash REDIS_SET 集合 set REDIS_ZSET 有序集合 zset Redis 中的键总是字符串对象 可以使用 type &lt;key&gt; 命令查看值是什么对象 编码在 redisObject 中提到 ptr 是指向底层数据结构，因为每种对象底层有多种实现方式，所以 ptr 到底指向什么结构是由 encoding 决定的，encoding 记录了对象当前的编码（是什么数据结构实现）。encoding 常量由下表所示： 常量 数据结构 REDIS_ENCODING_INT long 类型整数 REDIS_ENCODING_EMBSTR embstr 编码的 SDS REDIS_ENCODING_RAW SDS REDIS_ENCODING_HT 字典 REDIS_ENCODING_LINKEDLIST 双端链表 REDIS_ENCODING_ZIPLIST 压缩列表 REDIS_ENCODING_INTSET 整数集合 REDIS_ENCODING_SKIPLIST 跳跃表和字典 Redis 每个对象至少使用两种编码，如下表 类型 编码 对象实现方式 OBJECT ENCODING 命令输出 字符串 REDIS_STRING REDIS_ENCODING_INT 整数值 int REDIS_STRING REDIS_ENCODING_EMBSTR embstr 编码的简单动态字符串 embstr REDIS_STRING REDIS_ENCODING_RAW 简单动态字符串 raw 列表 REDIS_LIST REDIS_ENCODING_ZIPLIST 压缩列表 ziplist REDIS_LIST REDIS_ENCODING_LINKEDLIST 双端链表 linkedlist 哈希 REDIS_HASH REDIS_ENCODING_ZIPLIST 压缩列表 ziplist REDIS_HASH REDIS_ENCODING_HT 字典 hashtable 集合 REDIS_SET REDIS_ENCODING_INTSET 整数集合 intset REDIS_SET REDIS_ENCODING_HT 字典 hashtable 有序集合 REDIS_ZSET REDIS_ENCODING_ZIPLIST 压缩列表 ziplist REDIS_SET REDIS_ENCODING_SKIPLIST 跳跃表与字典 skiplist 可以使用 object encoding &lt;key&gt; 命令查看键对象的底层数据结构 使用 enconding 设定不同场景下不同的底层结构，有助于提升 redis 的灵活性和效率，例如，当一个列表元素较少时，底层使用压缩列表实现，压缩列表比双端链表更省空间，随着元素增多，使用压缩列表的优势慢慢消失，对象底层实现将转变成双端链表。 "},{"title":"JAVA 并发 —— 内存模型","date":"2020-09-20T07:36:09.000Z","url":"/2020/09/20/Java-memory-model/","tags":[["JAVA","/tags/JAVA/"],["并发","/tags/%E5%B9%B6%E5%8F%91/"]],"categories":[[" ",""]],"content":"并发编程模型的两个关键问题线程之间是如何通信和线程之间是如何同步。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。 其中共享内存是指，线程之间通过读-写内存中的公共状态进行隐式通信。而消息传递并发模型中，线程之间没有公共状态，线程之间必须通过发送消息来显示进行通信。 同步是指程序中用于控制不同线程间操作发生相对顺序的机制，在共享内存并发模型中，同步是显示进行的，程序必须显示指定某个方法或者某段代码需要在线程之间互斥执行。而在消息传递并发模型中，由于消息的发送必须在消息的接收之前，所以是隐式进行的。 Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。 Java 内存模型的抽象结构Java 中的实例域、静态域和数组元素都存储在堆内存中，堆内存是线程共享的。而局部变量、方法定义参数和异常处理参数不会在线程间共享。 Java 线程之间的通信由 Java 内存模型（JMM）控制，JMM 决定一个线程对共享变量的写入何时对另一个线程可见。如下图 JMM 抽象结构示意图。 上图中本地内存是一个抽象的概念，并不真实的存在，它包括缓存、写缓冲区、寄存器以及其它硬件和编译器优化。 如上图，如果线程A 要和 线程B 进行通信： 线程A 把本地内存A中更新过的共享变量刷新到主内存中。 线程B 到主内存中读取线程A之前更新过的共享变量 这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。 从源代码到指令序列的重排序在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。分为以下3种： 编译器优化的重排序。编译器不改变单线程程序语义前提下，可以重新安排语句的执行顺序。 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是乱序执行。 从 Java 源代码到最终指向的指令序列，分别经历上面的三种重排序。 上述1 属于编译器重排序，2、3属于处理器重排序。这些重排序可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。 指令重排序和内存屏障 假设处理器A和处理器B按程序顺序并行执行内存访问，最终可能结果为 x=y=0。具体原因如下图所示： 从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1→A2，但内存操作实际发生的顺序却是A2→A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样 ）。 为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为4类 ，如下图： StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。 happens-before 简介在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 happens-before 规则： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读 注意：两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens-before的定义很微妙 。 重排序重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 数据依赖性如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型 编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序 (只针对单线程情况下，多线程处理器和编译器不考虑数据依赖性) as-if-serial 语义as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。 为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。 如下示例： 上面操作的数据依赖如下： 其中 C 依赖于 A，C 依赖于 B，所以 C 不能被排序到 A 和 B 之前，但是 A 与 B 之间是不存在数据依赖的，所以 A 和 B 的顺序是可以被重排序的。以下是重排序后的结果： 程序顺序规则根据 happens-before 的程序顺序规则，上面的例子存在3个 happens-before 关系 A happens-before B B happens-before C A happens-before C 第3个 happens-before 关系是根据 happens-before 传递性推导出来的。虽然 A happens-before B，但是 B 可能在 A 前面执行。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。 重排序对多线程的影响 假设有两个线程A和B，A首先执行writer()方法，随后B线程接着执行reader()方法。线程B在执行操作4时，不一定能看到线程A在操作1对共享变量a的写入 。由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 如果操作1 和操作 2 重拍序，如下图： 操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还没有被线程A写入，在这里多线程程序的语义被重排序破坏了。 当操作3和操作4重排序时会产生什么效果 ，如下图： 操作3和操作4存在__控制依赖__关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（Reorder Buffer，ROB）的硬件缓存中。当操作3的条件判断为真时，就把该计算结果写入变量i中。如上图，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义。 在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。 顺序一致性顺序一致性内存模型是一个理论参考模型，在设计的时候，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。 数据竞争与顺序一致性Java内存模型规范对数据竞争的定义如下：在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序。 代码中包含数据竞争时，程序执行结果往往与预测的结果不一致。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。 JMM对正确同步的多线程程序的内存一致性做了如下保证。如果程序是正确同步的，程序的执行将具有顺序一致性（Sequentially Consistent）——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。马上我们就会看到，这对于程序员来说是一个极强的保证。这里的同步是指广义上的同步，包括对常用同步原语（synchronized、volatile和final）的正确使用。 顺序一致性内存模型顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性。 一个线程中的所有操作必须按照程序的顺序来执行。 （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。 顺序一致性内存模型为程序员提供的视图如下图 在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面图可以看出，在任意时间点最多只能有一个线程可以连接到内存。 这样把所有线程的所有内存读/写操作串行化。 假设有两个线程A和B并发执行。其中A线程有3个操作，它们在程序中的顺序是：A1→A2→A3。B线程也有3个操作，它们在程序中的顺序是：B1→B2→B3。 假设这两个线程使用监视器锁来正确同步：A线程的3个操作执行后释放监视器锁，随后B线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图示。 再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图 未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程A和B看到的执行顺序都是：B1→A1→A2→B2→A3→B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。 在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，当前线程写过的数据缓存在本地内存中，并没有刷新到主内存之前，这个写操作只对当前线程可见；从其他线程角度来看，这个写操作根本就没有执行。只有当前线程把本地内存写过的数据刷新到主内存中后，这个操作对其他线程才可见。这个时候，每个线程看到的执行顺序就不一致了。 同步程序的顺序一致性结果 假设A线程执行writer()方法后，B线程执行reader()方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。 顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程A在临界区内做了重排序，但由于监视器互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。 JMM在具体实现上的基本方针为：在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门。 未同步程序的执行特性对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，Null，False），JMM保证线程读操作读取到的值不会无中生有（Out Of Thin Air）的冒出来。为了实现最小安全性，JVM在堆上分配对象时，首先会对内存空间进行清零，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在已清零的内存空间（Pre-zeroed Memory）分配对象时，域的默认初始化已经完成了 。 JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。因为如果想要保证执行结果一致，JMM需要禁止大量的处理器和编译器的优化，这对程序的执行性能会产生很大的影响。而且，未同步程序在顺序一致性模型中，整体是无序的且结果无法预知，所以保证未同步程序在两个模型中执行结果一致也没什么意义。 未同步程序在两个模型中的执行特性有如下几个差异： 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序 JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。 在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（Bus Transaction）。总线事务包括读事务（Read Transaction）和写事务（Write Transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其他的处理器和I/O设备执行内存的读/写。 假设处理器A，B和C同时向总线发起总线事务，这时总线仲裁（Bus Arbitration）会对竞争做出裁决，这里假设总线在仲裁后判定处理器A在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器A继续它的总线事务，而其他两个处理器则要等待处理器A的总线事务完成后才能再次执行内存访问。假设在处理器A执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器D向总线发起了总线事务，此时处理器D的请求会被总线禁止。 总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行。在任意时间点，最多只能有一个处理器可以访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。 在一些32位的处理器上，如果要求对64位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的写操作具有原子性。当JVM在这种处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行。这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。 当单个内存操作不具有原子性时，可能会产生意想不到后果。如下图 如上图，假如一个处理器A写一个Long整型变量，64位写操作被分成两个32位的写操作，且分配到不同事务上执行。同时处理器B的64位读操作被分配到同一个事务中执行。那么B只能读到A写了一半的无效值。 从 JDK5 开始只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作都必须具有原子性（即任意读操作必须要在单个读事务中执行）。"},{"title":"Java 并发 —— Volatile","date":"2020-09-20T07:15:45.000Z","url":"/2020/09/20/volatile/","tags":[["并发","/tags/%E5%B9%B6%E5%8F%91/"],["Java","/tags/Java/"],["volatile","/tags/volatile/"]],"categories":[[" ",""]],"content":"volatile 是轻量级的 synchronized，它在多处理器开发中保证共享变量的可见性，可见性是指当一个线程修改一个共享变量时，另一个线程能够读取到修改后的值。volatile 的执行成本比 sychronized 更低，因为 volatile 不会引起线程的上下文切换。 1.1 volatile 的定义与原理在连接 volatile 的原理之前，先看看实现原理相关的 CPU 术语 volatile 是怎么保证可见性的呢？通过获取 JIT 编译器生成的汇编指令来查看 Java 代码 instance = new Singleton(); // instance 是 volatile 变量 转换为汇编后 有 volatile 修饰的共享变量在进行写操作时会多出第二行汇编代码，第二行代码是一个 Lock 前缀的指令，在多核处理器下会引发： 将当前处理器缓存行的数据写会到系统内存。 这个写会内存操作会使其它处理器缓存了该内存地址的数据无效 为了提高处理速度，处理器会将系统内存的数据读到高速缓存中（L1、L2），但是对高速缓存操作后不知道什么时候写回到内存中，如果对用 volatile 修饰的变量进行写操作后，JVM 会向处理器发送一条 Lock 前缀的指令，这时就会将修改后的变量写回到内存中，但是，即使将变量写回后，在多处理器的情况下，其它处理器缓存的还是旧的值，所以就有了 缓存一致性协议 。每个处理器通过嗅探总线上传播的数据来检查自己的缓存值是不是过期了，如果发现自己缓存行对应的内存地址进行过修改，那就将自己的缓存行设置为无效，下次进行操作时，再到内存中读到缓存中。 volatile 的两条实现原则 Lock 前缀指令会引起处理器缓存写到内存 以前，在多处理器环境下，Lock# 信号在声言该信号期间，处理器会独占共享内存，对于 Intel486 和奔腾系列处理器，在锁操作时，总是在总线上声言 Lock# 信号。但是，在最近的处理器中 Lock# 信号一般不锁总线，而是锁定缓存。如果需要访问的内存区域已经缓存在处理器内部，则不会声言 Lock# 信号，它会锁定这块内存区域的缓存并写回到处理器内部，并使用缓存一致性机制来保证修改的原子性，称为 缓存锁定 ，缓存一致性会阻止同时修改两个及以上处理器缓存的内存区域数据。 一个处理器将缓存写回到内存导致其它处理器缓存的数据失效 IA-32处理器和Intel 64处理器使用MESI（修改、独占、共享、无效）控制协议去维护内部缓存和其他处理器缓存的一致性。在多核处理器系统中进行操作的时候，IA-32和Intel 64处理器能嗅探其他处理器访问系统内存和它们的内部缓存。处理器使用嗅探技术保证它的内部缓存、系统内存和其他处理器的缓存的数据在总线上保持一致。 volatile 的使用优化在 JDK7 的并发包中新增了一个队列集合类 LinkedTransferQueue，代码如下 它使用内部类来定义头节点（head）和尾节点（tail），这个内部类相对于父类只是将共享变量追加到 64 字节。一个对象引用占 4 个字节，追加 15 个变量（60 字节），再加上父类的 value 变量，一共是64 字节。因为现在主流的处理器的 L1、L2、L3的高速缓存行是64字节宽，不支持充分填充行，所以如果头结点和尾节点都不足64字节时，处理器会将他们读到一个缓存行中，多处理器下会缓存同样的头、尾节点，当一个处理器试图修改头节点时，会将整个缓存行锁定，那么在缓存一致性协议下，其它的处理器不能访问自己缓存的尾节点，严重影响效率。 以下两种情况使用 volatile 变量时不应该追加到64字节 缓存行非64字节宽的处理器。 共享变量不会被频繁写，因为追加字节会导致处理器要读取更多的字节到高速缓存中，会消耗更多的性能。 volatile 的特性理解 volatile 特性的一个好方法就是把对 volatile 变量的单个读写看成是使用同一个锁对这些单个读/写操作做了同步。 如果多个线程分别调用上面程序的3个方法，这个程序的语义和下面的程序等价 可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作不具有原子性 volatile写-读建立的happens-before关系从内存语义的角度来说，volatile的写-读与锁的释放-获取有相同的内存效果：volatile写和锁的释放有相同的内存语义；volatile读与锁的获取有相同的内存语义。 假设线程A执行writer()方法之后，线程B执行reader()方法。根据happens-before规则，这个过程建立的happens-before关系可以分为3类： 根据程序次序规则，1 happens-before 2;3 happens-before 4 根据volatile规则，2 happens-before 3 根据happens-before的传递性规则，1 happens-before 4 volatile写-读的内存语义当写一个 volatile 变量时，JMM 会把该线程对应的本地内存中的共享变量值刷新到主内存。 当读一个volatile 变量时，JMM 会把该线程对应的本地内存置为无效，线程接下来将从主内存中读取共享变量。 volatile内存语义的实现下表是JMM针对编译器制定的volatile重排序规则表 当第一个操作为普通变量的读或写时，如果第二个操作为volatile写，则编译器不能重排序这两个操作，可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。 当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保volatile写之前的操作不会被编译器重排序到volatile写之后 当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保volatile读之后的操作不会被编译器重排序到volatile读之前 当第一个操作是volatile写，第二个操作是volatile读时，不能重排序 为了实现volatile的内存语义，编译器在生成字节码时，会在指令序列中插入内存屏障来禁止特定类型的处理器重排序。对于编译器来说，发现一个最优布置来最小化插入屏障的总数几乎不可能。为此，JMM采取保守策略 在每个volatile写操作的前面插入一个StoreStore屏障。 在每个volatile写操作的后面插入一个StoreLoad屏障。 在每个volatile读操作的后面插入一个LoadLoad屏障。 在每个volatile读操作的后面插入一个LoadStore屏障。 下面是 volatile 写插入内存屏障后生成的指令序列示意图 StoreStore屏障可以保证在volatile写之前，其前面的所有普通写操作已经对任意处理器可见了。这是因为StoreStore屏障将保障上面所有的普通写在volatile写之前刷新到主内存。 这里比较有意思的是，volatile写后面的StoreLoad屏障。此屏障的作用是避免volatile写与后面可能有的volatile读/写操作重排序。因为编译器常常无法准确判断在一个volatile写的后面是否需要插入一个StoreLoad屏障（比如，一个volatile写之后方法立即return）。为了保证能正确实现volatile的内存语义，JMM在采取了保守策略：__在每个volatile写的后面，或者在每个volatile读的前面插入一个StoreLoad屏障__。从整体执行效率的角度考虑，JMM最终选择了在每个volatile写的后面插入一个StoreLoad屏障。因为volatile写-读内存语义的常见使用模式是：一个写线程写volatile变量，多个读线程读同一个volatile变量。当读线程的数量大大超过写线程时，选择在volatile写之后插入StoreLoad屏障将带来可观的执行效率的提升。从这里可以看到JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。 下面是 volatile 读插入内存屏障后生成的指令序列示意图 在实际执行时，只要不改变volatile写-读的内存语义，编译器可以根据具体情况省略不必要的屏障。例如下面这个例子 针对readAndWrite()方法，编译器在生成字节码时可以做如下的优化 注意，最后的StoreLoad屏障不能省略。因为第二个volatile写之后，方法立即return。此时编译器可能无法准确断定后面是否会有volatile读或写，为了安全起见，编译器通常会在这里插入一个StoreLoad屏障。 "},{"title":"Git 基本使用","date":"2020-09-14T11:44:09.000Z","url":"/2020/09/14/Git/","tags":[["git","/tags/git/"],["tool","/tags/tool/"]],"categories":[[" ",""]],"content":"主要参考廖雪峰的官方网站-Git教程，删减上下文信息，把一些命令提取出来进行了简单的解释 基本操作 添加文件到仓库 查看工作区状态 版本回退 工作区和版本库 撤销修改 小结 场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，就回到了场景1，第二步按场景1操作。 场景3：已经提交了不合适的修改到版本库时，想要撤销本次提交，参考[版本回退]，不过前提是没有推送到远程库。 删除文件 先手动删除文件，然后使用git rm 和git add效果是一样的 远程仓库 添加远程库 克隆远程仓库 分支管理 创建与合并分支 解决冲突 解决冲突 分支管理策略 上面讲到了合并分支的 Fast forward 方式，在这种模式下，删除分支后，会丢掉分支信息，如果强制禁用 Fast forward 模式，git 在 merge 时会生成一个新的 commit，就能保留分支信息 好的分支管理策略应该像下图一样： bug 分支 当 master 分支出现 bug 需要紧急修复时，需要创建新的 bug 分支进行修复，而自己的工作又只做到一半需要保存时，先把工作现场 修复 master 分支后发现当前工作的分支也有这个 bug，现在只需用 删除没有合并的分支 如果有一个实验性质没有合并的分支需要删除需要 多人协作 与远程分支建立联系 一般多人协作的模式如下： 首先，可以试图用git push origin 推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin 推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch –set-upstream-to origin/。 Rebase 理想状态下希望提交记录是一条直线 可以通过 rebase 实现，一般把 rebase 叫变基，概念比较抽象，用下面几张图来解释a 初始情况： 使用 git rebase master 后，bugFix 分支上的工作在 master 的最顶端 然后切换到 master 分支，进行 git rebase bugFix 这里扩展一下 git pull 和 git pull –rebase 的区别 参考：简单对比git pull 和 git pull –rebase 的使用 同时还有一个经常使用的 git rebase -i ，这个命令可以选取你连续提交的多次进行合并。 标签管理"},{"title":"正则表达式-基础","date":"2020-09-14T11:44:09.000Z","url":"/2020/09/14/regular-expressions/","tags":[["regular","/tags/regular/"],["正则表达式","/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"]],"categories":[[" ",""]],"content":" 开头结尾^a 匹配的字符串以 a 开头（括号里除外） a$ 匹配字符串已 a 结尾 字符 [abc]，表示匹配中括号中的任意字符 [a-zA-Z0-9]，范围表示 [^a-z]，取反，表示不等于 a-z 中任意字符 .，表示任意字符（\\n 除外） \\s，表示空白字符，等同于 [\\r\\n\\t\\f\\v ] \\S，表示除了空白字符，即 [^\\r\\n\\t\\f\\v ] \\w，匹配字母数字下划线，等同于 [a-zA-Z0-9_] \\W，\\w 取反 \\d，匹配任意一个数字，即 [0-9] \\D，\\d 取反 上面几个转意匹配的，只要大小字母同时出现就可以匹配任意字符即 [\\s\\S] 就可以匹配任意字符。以上都是表示匹配单个字符，下面是匹配多个字符： a?，匹配 0 个或 1 个 a a*，匹配 0 个到无限个 a a+，匹配 1 个到无限个 a a&#123;m&#125;，匹配连续出现 m 次的 a a&#123;m,n&#125;，匹配连续出现 m - n 的 a a&#123;m,&#125;，匹配连续出现 m - 无穷的 a "},{"title":"JAVA8 新特性1 —— Lambda 表达式","date":"2020-09-14T09:15:45.000Z","url":"/2020/09/14/JAVA8-Lambda/","tags":[["JAVA","/tags/JAVA/"],["JAVA8","/tags/JAVA8/"],["Lambda","/tags/Lambda/"]],"categories":[[" ",""]],"content":"JAVA8的新特性核心是Lambda表达式和Steam API 一、Lambda表达式：1、语法：Java8中引入了一个新的操作符“-&gt;”，称为箭头操作符或Lambda操作符，箭头操作符将Lambda表达式拆分成两部分：左侧：Lambda表达式的参数列表右侧：Lambda表达式中所执行的功能，也称Lambda体 语法格式一：无参数，无返回值 语法格式二：有一个参数且无返回值 语法格式三：有两个及以上的参数，有返回值，并且Lambda体中有多条语句 语法格式四：如果Lambda体只有一条语句，大括号和 return 可以省略不写 注：Lambda 表达式的参数列表的数据类型可以不用写，JVM可以通过上下文可以自动推断数据类型。 2、Lambda 表达式需要“函数式接口”支持函数式接口：接口中只有一个抽象方法的接口，称为函数式接口。可以使用注解@FunctionalInterface 修饰检查是否为函数式接口用一个例子来说明： Lambda使用：①：声明一个函数式接口，接口中声明一个抽象方法。②：类中编写方法使用接口作为参数。③: 在调用②中方法时，接口参数部分使用 Lambda 表达式。 JAVA8 提供以下四大核心函数式接口 更多函数式接口可在java8官方文档中查看 Lambda 方法引用若Lambda体中的内容有方法已经实现了，可以用“方法引用”注意：lambda体中调用方法的参数列表与返回类型必须一致 1、类::静态方法名 2、对象::实例方法名 3、类::实例方法名若Lambda 参数列表的第一个参数是实例方法的调用者，第个参数是实例方法的参数是，可以使用ClassName::method 构造器引用ClassName :: new注意：需要调用的构造器的参数列表要与函数式接口中的抽象方法列表必须一致！ 数组引用格式：Type[]::new "},{"title":"JAVA NIO","date":"2020-09-13T11:44:09.000Z","url":"/2020/09/13/Java%20NIO/","tags":[["JAVA","/tags/JAVA/"],["NIO","/tags/NIO/"],["并发","/tags/%E5%B9%B6%E5%8F%91/"]],"categories":[[" ",""]],"content":"直接内存，间接内存&#8195;java.nio 从 Java 1.4开始引入，可以叫New I/O，也可叫Non-Blocking I/O。java.nio 有三个核心概念Selector、Channel、Buffer，在java.nio中我们是面向块（block）或者缓冲区（buffer）编程，而不是像 java.io 中的面向流编程。buffer 是内存中的一块区域，底层的实现是数组，所有数据的读或者写都是通过 buffer 来实现。 &#8195;对于Java的8中基本数据类型都有（除了 Boolean）对应的 Buffer 类型，如 ByteBuffer、CharBuffer、IntBuffer 等&#8195;Channel 是指可以写入或者读取数据的对象，类似于 java.io 中的 Stream，不过 Channel 是双向的，可以进行读写。但是所有的读写操作都是通过 Buffer 进行，不会直接通过 Channel 读写数据。 Buffer&#8195;Buffer 中有几个重要的属性 mark、position、limit、capacity，其中 0 &lt;= mark &lt;= position &lt;= limit &lt;= capacity。&#8195;capacity 表示缓冲区 Buffer 的容量，不能为负数。limit 为缓冲区的限制，不能为负，限制代表缓冲区中第一个不能读取或者写入元素的索引（下标）。&#8195;position 代表下一个要读取或者写入元素的索引（下标），不能为负。&#8195;mark 表示缓冲区的标记，标记的作用是调用 reset() 方法时，会将 position 位置重置到 mark 位置，标记不是必须的，而且标记不能大于 position，如果定义了 mark ，再将 position 或 limit 重置到比 mark 小的位置时会丢弃 mark，将 mark 置为 -1。如果未定义 mark 在调用 reset() 方法时会抛出 InvalidMarkException 异常。总结： 1 ）缓冲区的 capacity 不能为负数，缓冲区的 limit 不能为负数，缓冲区的 position 不能为负数 。2) position 不能大于其 limit 。3) limit 不能大于其 capacity 。4 ）如果定义了 mark ，则在将 position 或 limit 调整为小于该 mark 的值时，该 mark 被丢弃 。5 ）如果未定义 mark ，那么调用 reset（） 方法将导致抛出 InvalidMarkException 异常 。6 ）如果 position 大于新的 limit ，则 position 的值就是新 limit 的值 。7 ）当 limit 和 position 值一样时，在指定的 position 写入数据时会 出现异常，因为此位置是被限制的 。 flip() 方法例子： 上述例子中，将 niotest1.txt 读入 buffer 后进行了一次 flip 操作，下面是 flip 方法的源码。flip 操作将 limit 设置为当前的 position，下次读取操作时就不会超过赋值的界限，保证读取的数据都是有效的。然后 position 设置为 0，下次读取时能从下标 0 开始读，mark 设置为 -1 clear() 方法&#8195;clear 方法只是将 limit 设置为 capacity，position 设置为 0，并没有将数据删除，而只是将 buffer 数组设置为初始状态，下次写操作时直接覆盖，而读操作可以把原来的数据读出来。下面是 clear 方法的源码 相对位置和绝对位置ByteBuffer 类型化 put 和 get方法就是，将其他类型 put 进 ByteBuffer，但是，put 什么类型，get 就是什么类型，顺序不能变。 共享底层数组 slice 共享相同的数组直接缓冲和零拷贝 DirectBuffer 内存映射文件 MappedByteBuffer将文件的全部或者一部分映射到堆外内存中，Java即可以直接操作内存，而不用操作文件，减少I/O操作，提升操作效率 关于 Buffer 的 Scattering（分散）和 Gathering（收集）Scattering 是指在使用 Channel 进行读取的时候，如果我们传入的是一个 buffer 数组，那么会将第一个 buffer 读满后再读入第二个 buffer 依次进行。Gathering 是指写出的时候传入一个 buffer 数组，会将第一个 buffer 全部写出，再将第二个 buffer 全部写出，依次进行。 unicode 是编码方式utf 是存储方式utf-8 是unicode的实现方式"},{"title":"JAVA8 新特性2 —— Stream API","date":"2020-09-13T09:25:45.000Z","url":"/2020/09/13/JAVA8-Stream%20API/","tags":[["JAVA","/tags/JAVA/"],["Stream API","/tags/Stream-API/"],["JAVA8","/tags/JAVA8/"]],"categories":[[" ",""]],"content":"Steam API (java.util.stream.*) 三个步骤①：将数据源转换成流②：一系列中间操作③：产生一个新流（不改变源数据） 流（Stream）是什么？是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。**集合讲的是数据，流讲的是计算！** 注意：**①：Stream 自己不会存储元素。②：Stream 不会改变源对象。会返回一个处理后的新Stream。③：Stream 操作是延迟执行的。要等到需要结果的时候才执行** 一、创建 Stream 1、可以通过Collection 系列集合提供的 stream() (串形流) 或 parallelStream() (并行流)获取流 2、通过Arrays 中的静态方法 stream() 获取数组流 3、通过 Stream 类中的静态方法 of() 4、无限流 此时的流是没有任何效果的，只是被创建出来了，要通过中间操作或者终止操作才有效果 二、中间操作 中间操作，如果没有终止操作是不会有任何的执行 1、筛选与切片 filter：接收 Lambda，从流中排除某些元素 limit：截断流，使元素不超过给定数量。 skip(n)：跳过元素，返回一个扔掉前 n 个元素的流，若流中不足 n 个，返回空流，与 limit 互补 distinct：筛选，通过流所生成的 hashCode() 和 equals() 去除重复元素 终止操作：一次性执行全部内容，叫“惰性求值”或“延时加载“ 外部迭代 2、映射 map：接收 Lambda，将元素转换成其他形式或提取信息。接收一个函数作为参数，该函数会被应用每一个元素上，并将其映射成一个新的元素。 flatMap：接收一个函数作为参数，将流中的每个值都换成另一流，然后把所有流连接成一个流 map 案例 输出 flatMap 输出结果 map 和 flatMap 有点像 List 的 add 和 addAll 3、排序sorted：自然排序sorted(Comparator com)：订制排序 三、终止操作 1、查找与匹配allMatch：检查是否匹配所以元素anyMatch：检查是否至少匹配一个元素noneMatch：检查是否没有匹配所有元素findFirst：返回第一个元素findAny：返回当前流的任意元素count：返回流中元素的总个数max：返回流中最大值min：返回流中最小值 2、归约reduce(T identity, BinaryOperator)：将流中元素反复结合起来，得到一个值 3、收集collect：将流转换成其他形式，接收一个Collector接口实现，用于给Stream中元素做汇总的方法。 "},{"title":"ArrayList 源码解析","date":"2020-09-13T07:44:09.000Z","url":"/2020/09/13/ArrayList/","tags":[["JAVA","/tags/JAVA/"],["ArrayList","/tags/ArrayList/"],["集合","/tags/%E9%9B%86%E5%90%88/"],["源码","/tags/%E6%BA%90%E7%A0%81/"]],"categories":[[" ",""]],"content":"首先存储的数据结构为：transient Object[] elementData; 构造方法构造方法有三种： 无参构造方法：创建一个默认容量的空数组，但是这里用的是 目的是与 区分，知道第一次添加元素该扩容多少，挖坑，后续填 带初始容量的构造方法：在 new ArrayList 的时候指定一个整数参数，这个参数为初始容量大小，这里会做一个判断，分三种情况 参数正常，非负且大于0，创建一个大小为传入值的数组 参数为0，使用 EMPTY_ELEMENTDATA 异常 初始参数为集合，构造一个包含指定集合元素的列表，不过传入的列表的对象必须是 Collection&lt;? extends E&gt; 如果传入的集合没问题则创建一个元素为传入集合的List 如果传入的集合大小为0，则使用 EMPTY_ELEMENTDATA 添加元素添加元素的方法主要有四种 add(E e) 在添加元素之前要确保容量够，会根据一个原有 size + 1 生成的minCapacity 去进行比较，如果为空时，且是之前无参构造方法创建的对象（DEFAULTCAPACITY_EMPTY_ELEMENTDATA），会扩容到默认初始值为 10，如果是之前有参构造函数创建的方法，就判断 minCapacity 是否大于原来数组的长度，如果比原来数组长度小，即添加的元素要数组越界了，必须先扩容再添加。 首先扩容是原来容量的 1.5 倍，这里还有一个容量最大问题，如果容量超过了 MAX_ARRAY_SIZE，要么溢出，要么使用 Integer.MAX_VALUE，至于为什么 MAX_ARRAY_SIZE 为 nteger.MAX_VALUE - 8，官方说法是有些虚拟机在数组中保留了一些”header words”，需要给这些“header words”留一些空间。 中间还有一个 newCapacity - minCapacity &lt; 0 的判断，以我的理解应该是 oldCapacity 已经非常大了，再增长 1.5 倍就会溢出了，所以如果溢出了就增长 1 即可。 最后，添加元素 add(int index, E element) 指定位置插入 步骤与 1 基本一样，只是最后需要从 index开始都往后挪一个位置 再把插入的值放到 index 位置上。 addAll(Collection&lt;? extends E&gt; c) 增加一个集合 步骤与 1 也一样，只是确保容量的时候传的是原来 size + c.length，然后 addAll(int index, Collection&lt;? extends E&gt; c) 指定位置插入集合 步骤与 2 类似，只是将往后移动一个换成往后移动一段 删除元素 remove(int index) 先验证 index 是否合法，然后保存原来的值，最后 然后返回删除的值 remove(Object o) 这个方法是删除对象的，删除 List 中出现的第一个与参数相等的对象，且不返回删除值。这里如果传入 null 则用 == 进行比较，如果非空则用 equals 进行比较，找到相等的后，调用一个私有的不进行 index 检查，不返回值的 fastRemove方法 clear() 对数组每个元素置空，将 size 置0 removeAll(Collection&lt;?&gt; c) 采用覆盖的方式，对原数组进行遍历，判断 c 中是否包含，如果包含就直接覆盖 最后再对未被覆盖且不需要的元素置空，以便 GC retainAll(Collection&lt;?&gt; c) 这个方法与 removeAll 相反，是保留，实现方式与 removeAll 一样，只是 complement = true 查询数据 get(int index) 直接返回下标为 index 的元素 indexOf(Object o) 遍历数组，返回第一个等于 o 的元素的下标，否则返回 -1 lastIndexOf(Object o) 反向遍历数组，返回最后一个等于 o 的元素的下标，否则返回 -1 contains(Object o) 调用 indexOf()，判断返回值是否 &gt;=0 subList(int fromIndex, int toIndex) subList 返回一个子 List，但共用父 List，只是在每个操作时对下标添加一个 offset，特别是 add 操作，每次添加都要移动父 List 的元素，所以效率不高。 "},{"title":"Redis 基本数据结构","date":"2020-09-12T07:15:45.000Z","url":"/2020/09/12/redis-baseDataConstruct/","tags":[["Reids","/tags/Reids/"]],"categories":[[" ",""]],"content":"字符串Redis 没有使用 C 语言原生的字符串，而是重新定义了一种数据结构——SDS（Simple Dynamic String）简单动态字符串，数据结构如下 free : 表示 buf 中可用的字节空间 len : 表示字符串的长度（不包括结束符） buf : 字节数组，用于存放二进制字节 free 用于每次拼接前 SDS 较 C 语言原生字符串有何优势： 获取字符串长度时不用遍历整个数组，直接读取 len 长度即可，时间复杂度为 O(1)，C 语言中获取字符串长度需要遍历整个数组，时间复杂度为 O(n) 。 字符串拼接时不会出现内存溢出： C 字符串在做字符串拼接之前需要先手动进行内存重分配，再进行拼接，很容易遗忘这个步骤造成内存溢出；而 SDS 每次进行拼接前先判断 free 的长度是否够拼接的长度，如果不够，先进行扩容。 减少修改字符串时的内存重分配次数 C 字符串每次对字符串进行增长或缩短都需要对内存进行重分配。 SDS 采用空间预分配和惰性空间释放的方式 如果对 SDS 修改后，SDS 的空间未超过 1MB，则会分配和 len 属性同样大小的未使用空间，这时 len == free，例如：如果修改后 SDS 的大小为 20 字节，则会分配 20 字节的free 空间，此时 len == free == 20，buf == 20 + 20 + 1 (1 字节用于保存空字符) 如果 SDS 修改后大于等于 1MB ，则会分配 1MB 的 free 空间，例如：SDS 修改后为 20 MB，则会分配 1MB 的 free 空间，此时，len == 30MB，free == 1MB，buf == 30MB + 1MB + 1byte(1 字节用于保存空字符) 在对 SDS 进行缩短操作后，不会马上释放空间，而是保存在 buf 里，如果以后做增长操作就能用上。再内存不足或者其他真正需要释放时就会进行释放。 二进制安全 C 字符串的字符必须符合某种编码（例如 ASCLL），并且处理结尾其它位置都不能出现空字符。而 SDS 是二进制安全的，就是在保存和读出的时候不对内容做任何操作，如过滤、筛选等等，存入的是什么，读出来就是什么。 兼容 C 字符串的部分函数 SDS 遵循 C 字符串中的以空字符串结尾的方式，所以 SDS 可以重用 &lt;string.h&gt; 库的部分函数，如 strcasecmp(sds-&gt;buf, &quot;hello world&quot;)、strcat(c_string, sds-&gt;buf)等。 总结：SDS 与 C 字符的对比 C 字符串 SDS 获取字符串长度的复杂度为 O(N) 。 获取字符串长度的复杂度为 O(1) 。 API 是不安全的，可能会造成缓冲区溢出。 API 是安全的，不会造成缓冲区溢出。 修改字符串长度 N 次必然需要执行 N 次内存重分配。 修改字符串长度 N 次最多需要执行 N 次内存重分配。 只能保存文本数据。 可以保存文本或者二进制数据。 可以使用所有 &lt;string.h&gt; 库中的函数。 可以使用一部分 &lt;string.h&gt; 库中的函数。 链表Redis 的链表由 链表(list)和链表节点(listNode)组成。 Redis 链表的特点如下： 双端： 链表节点带有 prev 和 next 指针， 获取某个节点的前置节点和后置节点的复杂度都是 O(1) 。 无环： 表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL ， 对链表的访问以 NULL 为终点。 带表头指针和表尾指针： 通过 list 结构的 head 指针和 tail 指针， 程序获取链表的表头节点和表尾节点的复杂度为 O(1) 。 带链表长度计数器： 程序使用 list 结构的 len 属性来对 list 持有的链表节点进行计数， 程序获取链表中节点数量的复杂度为 O(1) 。 多态： 链表节点使用 void* 指针来保存节点值， 并且可以通过 list 结构的 dup 、 free 、 match 三个属性为节点值设置类型特定函数， 所以链表可以用于保存各种不同类型的值。 字典 基本数据结构首先看下 Redis 中字典的哈希表： 其中： table 属性是一个数组，数组中的元素是指向 dictEntry 的指针，dictEntry 中保存一个键值对 上图是一个大小为 4 的空的哈希表。 然后我们看下哈希表节点： 其中： val 可以是一个指针，也可以是 uint64_t 或 int64_t 整数 next 是用来指向下一个节点的指针，用于解决冲突，没错，reids 哈希表用于解决冲突的方式就是链地址法 然后我们看下 Redis 字典的结构： 其中 ： type 和 privdata 属性是针对不同类型的键值对 ht 属性是大小为 2 的数组，一般只用 ht[0]，ht[1] 在 rehash 的时候才会使用 rehashidx 是在 rehash 时记录下标的，后面讲 rehash 的时候会用到 最后，看下 dictType 的实现： 上图是一个普通为进行 rehash 的字典 哈希算法我们将一个 &lt;k, v&gt; 对添加到字典里的步骤是： 计算 k 的 哈希值 根据哈希值，通过 sizemask 计算除索引值 根据索引放入哈希表数组中 Redis 计算哈希值和索引的方式： 注：Redis 使用 MurmurHash2 算法计算键的哈希值 解决哈希冲突Redis 的哈希表采用链地址法（separate chaining）来解决冲突 因为 dictEntry 组成的链表没有指向表尾的指针，所有为了考虑速度，总是将新节点添加到表头的位置 rehash在容量固定的情况下，性能会随着 &lt;K, V&gt; 断增多而下降，因为， &lt;K, V&gt; 越来越多，造成哈希冲突的情况越来越多，dictEntry 链表越来越长，导致每次取值都要对链表进行遍历，所以这种情况就需要扩容; 另一种情况就是，哈希表有一个很大的容量，而里面的 &lt;K, V&gt; 越来越少（一开始不断扩容，随着使用，不断删除里面的 &lt;K, V&gt; ），这时为了避免空间浪费就需要收缩。而扩容与收缩都是通过 rehash （重新散列）来实现。rehash 步骤如下： 为字典中的 ht[1] 分配空间，这里空间分配的大小取决于执行的操作 如果是扩容，ht[1] 的大小为第一个大于等于 ht[0].used * 2 的 2^n （ 2 的 n 次幂），假设 ht[0].used == 4，那么 4 * 2 = 8，2 的 3 次方刚好是 8，所以 ht[1] 大小就为 8 如果是收缩，ht[1] 的大小为第一个大于等于 ht[0].used 的 2^n （ 2 的 n 次幂） 将 ht[0] 的 &lt;K, V&gt; rehash 到 ht[1] 上，rehash 就是重新计算哈希值和索引值 迁移完后，释放 ht[0] , 将 ht[1] 设置为 ht[0] 对上图进行扩容步骤 先分配空间：ht[0].used 当前的值为 4 ， 4 * 2 = 8 ， 而 8 （2^3）恰好是第一个大于等于 4 的 2 的 n 次方， 所以程序会将 ht[1] 哈希表的大小设置为 8 将 ht[0] 的 &lt;K, V&gt; rehash 到 ht[1] 上： 重置指针，释放空间： 什么时候进行扩容？以下两个条件满足其一就会进行扩容 服务器没有在执行 BGSAVE 命令或 BGREWRITEAOF 命令的时候，负载因子大于等于 1 时 服务器正在执行 BGSAVE 命令或 BGREWRITEAOF 命令的时候，负载因子大于等于 5 时 （BGSAVE 和 BGREWRITEAOF 命令是 Redis 的持久化相关的命令） 负载因子计算公式 渐进式 rehash上面说到了 rehash 的过程，将 4 个 &lt;K, V&gt; rehash 可以很快完成，但是如果 &lt;K, V&gt; 是百万级、千万级、亿级呢？如果一次将 ht[0 中所有 &lt;K, V&gt; rehash 到 ht[1]，那这个计算量会导致服务停止服务一段时间，直到 rehash 完成。 所以，rehash 并不是一次性完成的，而是分多次、渐进式的，渐进式 rehash 的步骤： 为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表 在字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始 在 rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一 随着字典操作的不断执行， 最终在某个时间点上， ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成 例如下面这个例子： 准备 rehash rehash 索引 0 上的 &lt;K, V&gt; rehash 索引 1 上的 &lt;K, V&gt; 依次 rehash 3、4 上的索引，在进行渐进式 rehash 的过程中，删除、查找、更新等操作都会在 ht[0] 和 ht[1] 两个哈希表上，但是每次添加都是添加到 ht[1] 中 跳表这篇跳表的文章写得很不错Skip List–跳表（全网最详细的跳表文章没有之一) 整数集合 数据结构 整数集合的所有元素都保存在 contents 数组中，这些元素按从小到大有序且不重复排列，虽然这里声明的 contents 是 int8_t 类型的，但是其真正类型取决于 encoding 如果 encoding 属性的值为 INTSET_ENC_INT16 ， 那么 contents 就是一个 int16_t 类型的数组， 数组里的每个项都是一个 int16_t 类型的整数值 （最小值为 -32,768 ，最大值为 32,767 ）。 如果 encoding 属性的值为 INTSET_ENC_INT32 ， 那么 contents 就是一个 int32_t 类型的数组， 数组里的每个项都是一个 int32_t 类型的整数值 （最小值为 -2,147,483,648 ，最大值为 2,147,483,647 ）。 如果 encoding 属性的值为 INTSET_ENC_INT64 ， 那么 contents 就是一个 int64_t 类型的数组， 数组里的每个项都是一个 int64_t 类型的整数值 （最小值为 -9,223,372,036,854,775,808 ，最大值为 9,223,372,036,854,775,807 ）。 如上图示例中 encoding 保存的是 INTSET_ENC_INT16 且数组长度为 5 的，所以 contents 占 16 * 5 = 80位，encoding 的编码取决于数组中最大的元素，也就是会出现这种情况，数组中只有一个元素是 INTSET_ENC_INT64 类型，其它都是 INTSET_ENC_INT16 类型，也要按照 INTSET_ENC_INT64 编码 升级当一个新元素要添加到数组里时，并且新元素比所有原来的元素都要长时，就要进行升级(upgrade)，升级共分三步： 根据新元素大小，扩展数组大小，并为新元素分配空间 将数组现有元素都转换成新元素类型，并放置到正确位置，且保证有序性 将新元素添加到数组中 比如上图的例子，每个元素都占 16 位，数组长度为 48 位 如果现在要添加一个 int32_t 类型的 65535 到集合里，此时就要进行升级，先要根据类型和元素个数计算分配的空间，(3 + 1) * 32 = 128 位 分配完空间后，对原有元素进行转换，因为元素 3 在 1、2、3、65535 中排第三，所以移动到索引 2 的位置上 最后升级完成后的结构 因为每次向集合添加新元素都有可能会引起升级，每次升级都要对元素进行转换，所以添加新元素的复杂度为 O(n)。 升级后新元素的位置 因为引发升级操作的新元素比现有所有元素都要大，所以这个新元素要么大于所有元素（索引为 length - 1），要么小于所有元素（索引为 0） 为什么要设计为升级 灵活 因为 C 语言是静态类型语言，般只使用 int16_t 类型的数组来保存 int16_t 类型的值， 只使用 int32_t 类型的数组来保存 int32_t 类型的值， 诸如此类。自动升级操作可以将 int16_t 、 int32_t 或者 int64_t 类型的整数添加到集合中，不用担心类型错误，非常灵活。 节约内存 如果要让一个数组同时能够保存 int16_t 、 int32_t 、int64_t 类型的元素，最简单的就是直接使用 int64_t 类型，有可能浪费内存。 降级 不会对升级后的数组进行降级 压缩列表压缩列表是为了节约内存而开发的 数据结构 "},{"title":"搜索","date":"2021-07-07T12:29:42.375Z","url":"/search/index.html","categories":[[" ",""]]},{"title":"标签云","date":"2021-07-07T12:29:42.375Z","url":"/tags/index.html","categories":[[" ",""]]}]