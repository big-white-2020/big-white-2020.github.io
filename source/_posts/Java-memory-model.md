---
title : JAVA 并发 —— 内存模型
date : 2020-09-20 15:36:09
tag : [JAVA, 并发]
---

##### 并发编程模型的两个关键问题

线程之间是如何通信和线程之间是如何同步。在命令式编程中，线程之间的通信机制有两种：共享内存和消息传递。

其中共享内存是指，线程之间通过读-写内存中的公共状态进行隐式通信。而消息传递并发模型中，线程之间没有公共状态，线程之间必须通过发送消息来显示进行通信。

同步是指程序中用于控制不同线程间操作发生相对顺序的机制，在共享内存并发模型中，同步是显示进行的，程序必须显示指定某个方法或者某段代码需要在线程之间互斥执行。而在消息传递并发模型中，由于消息的发送必须在消息的接收之前，所以是隐式进行的。

Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对程序员完全透明。

##### Java 内存模型的抽象结构

Java 中的实例域、静态域和数组元素都存储在堆内存中，堆内存是线程共享的。而局部变量、方法定义参数和异常处理参数不会在线程间共享。

Java 线程之间的通信由 Java 内存模型（JMM）控制，JMM 决定一个线程对共享变量的写入何时对另一个线程可见。如下图 JMM 抽象结构示意图。

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511161338.png)

上图中本地内存是一个抽象的概念，并不真实的存在，它包括缓存、写缓冲区、寄存器以及其它硬件和编译器优化。

如上图，如果线程A 要和 线程B 进行通信：

1. 线程A 把本地内存A中更新过的共享变量刷新到主内存中。
2. 线程B 到主内存中读取线程A之前更新过的共享变量

这两个步骤实质上是线程A在向线程B发送消息，而且这个通信过程必须要经过主内存。JMM通过控制主内存与每个线程的本地内存之间的交互，来为Java程序员提供内存可见性保证。  

##### 从源代码到指令序列的重排序

在执行程序时，为了提高性能，编译器和处理器常常会对指令做重排序。分为以下3种：

1. 编译器优化的重排序。编译器不改变单线程程序语义前提下，可以重新安排语句的执行顺序。
2. 指令级并行的重排序。现代处理器采用了指令级并行技术来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序。
3. 内存系统的重排序。由于处理器使用缓存和读/写缓冲区，这使得加载和存储操作看上去可能是乱序执行。

从 Java 源代码到最终指向的指令序列，分别经历上面的三种重排序。

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511163300.png)

上述1 属于编译器重排序，2、3属于处理器重排序。这些重排序可能会导致多线程程序出现内存可见性问题。对于编译器，JMM的编译器重排序规则会禁止特定类型的编译器重排序（不是所有的编译器重排序都要禁止）。对于处理器重排序，JMM的处理器重排序规则会要求Java编译器在生成指令序列时，插入特定类型的内存屏障Memory Barriers，Intel称之为Memory Fence）指令，通过内存屏障指令来禁止特定类型的处理器重排序。

##### 指令重排序和内存屏障

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511164438.png)

假设处理器A和处理器B按程序顺序并行执行内存访问，最终可能结果为 x=y=0。具体原因如下图所示：

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511164553.png)

从内存操作实际发生的顺序来看，直到处理器A执行A3来刷新自己的写缓存区，写操作A1才算真正执行了。虽然处理器A执行内存操作的顺序为：A1→A2，但内存操作实际发生的顺序却是A2→A1。此时，处理器A的内存操作顺序被重排序了（处理器B的情况和处理器A一样 ）。

为了保证内存可见性，Java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定类型的处理器重排序。JMM把内存屏障指令分为4类 ，如下图：

![image-20200920114754092](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/image-20200920114754092.png)

StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。  

##### happens-before 简介

在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这两个操作之间必须要存在happens-before关
系。这里提到的两个操作既可以是在一个线程之内，也可以是在不同线程之间。 

happens-before 规则：

1. 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。
2. 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。
3. volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读  

注意：两个操作之间具有happens-before关系，并不意味着前一个操作必须要在后一个操作之前执行！happens-before仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前（the first is visible to and ordered before the second）。happens-before的定义很微妙  。

#### 重排序

重排序是指编译器和处理器为了优化程序性能而对指令序列进行重新排序的一种手段。 

##### 数据依赖性

如果两个操作访问同一个变量，且这两个操作中有一个为写操作，此时这两个操作之间就存在数据依赖性。数据依赖分为下列3种类型  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511165602.png)

编译器和处理器在重排序时，会遵守数据依赖性，编译器和处理器不会改变存在数据依赖关系的两个操作的执行顺序 <font color=red>(只针对单线程情况下，多线程处理器和编译器不考虑数据依赖性)</font>

##### as-if-serial 语义

as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不能被改变。编译器、runtime和处理器都必须遵守as-if-serial语义。  

为了遵守as-if-serial语义，编译器和处理器不会对存在数据依赖关系的操作做重排序，因为这种重排序会改变执行结果。但是，如果操作之间不存在数据依赖关系，这些操作就可能被编译器和处理器重排序。  如下示例：

```
double pi = 3.14; // A
double r = 1.0; // B
double area = pi * r * r; // C
```

上面操作的数据依赖如下：

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511170051.png)

其中 C 依赖于 A，C 依赖于 B，所以 C 不能被排序到 A 和 B 之前，但是 A 与 B 之间是不存在数据依赖的，所以 A 和 B 的顺序是可以被重排序的。以下是重排序后的结果：

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511170230.png)

##### 程序顺序规则

根据 happens-before 的程序顺序规则，上面的例子存在3个 happens-before 关系

1. A happens-before B
2. B happens-before C
3. A happens-before C

第3个 happens-before 关系是根据 happens-before 传递性推导出来的。虽然 A happens-before B，但是 B 可能在 A 前面执行。如果A happens-before B，JMM并不要求A一定要在B之前执行。JMM仅仅要求前一个操作（执行的结果）对后一个操作可见，且前一个操作按顺序排在第二个操作之前。这里操作A的执行结果不需要对操作B可见；而且重排序操作A和操作B后的执行结果，与操作A和操作B按happens-before顺序执行的结果一致。在这种情况下，JMM会认为这种重排序并不非法（not illegal），JMM允许这种重排序。

##### 重排序对多线程的影响

```
class ReorderExample {
	int a = 0;
	boolean flag = false;
	public void writer() {
		a = 1; 		 		// 1
		flag = true; 		// 2
	}
	public void reader() {
		if (flag) { 		// 3
			int i = a * a; 	// 4
		}
	}
}
```

假设有两个线程A和B，A首先执行writer()方法，随后B线程接着执行reader()方法。线程B在执行操作4时，不一定能看到线程A在操作1对共享变量a的写入 。由于操作1和操作2没有数据依赖关系，编译器和处理器可以对这两个操作重排序；同样，操作3和操作4没有数据依赖关系，编译器和处理器也可以对这两个操作重排序。 如果操作1 和操作 2 重拍序，如下图： 

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511171537.png)

操作1和操作2做了重排序。程序执行时，线程A首先写标记变量flag，随后线程B读这个变量。由于条件判断为真，线程B将读取变量a。此时，变量a还没有被线程A写入，在这里多线程程序的语义被重排序破坏了。

当操作3和操作4重排序时会产生什么效果 ，如下图：

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511171659.png)

操作3和操作4存在__控制依赖__关系。当代码中存在控制依赖性时，会影响指令序列执行的并行度。为此，编译器和处理器会采用猜测（Speculation）执行来克服控制相关性对并行度的影响。以处理器的猜测执行为例，执行线程B的处理器可以提前读取并计算a*a，然后把计算结果临时保存到一个名为重排序缓冲（Reorder Buffer，ROB）的硬件缓存中。当操作3的条件判断为真时，就把该计算结果写入变量i中。如上图，猜测执行实质上对操作3和4做了重排序。重排序在这里破坏了多线程程序的语义。

在单线程程序中，对存在控制依赖的操作重排序，不会改变执行结果（这也是as-if-serial语义允许对存在控制依赖的操作做重排序的原因）；但在多线程程序中，对存在控制依赖的操作重排序，可能会改变程序的执行结果。  

#### 顺序一致性

顺序一致性内存模型是一个理论参考模型，在设计的时候，处理器的内存模型和编程语言的内存模型都会以顺序一致性内存模型作为参照。

##### 数据竞争与顺序一致性

Java内存模型规范对数据竞争的定义如下：
在一个线程中写一个变量，在另一个线程读同一个变量，而且写和读没有通过同步来排序。  

代码中包含数据竞争时，程序执行结果往往与预测的结果不一致。如果一个多线程程序能正确同步，这个程序将是一个没有数据竞争的程序。

JMM对正确同步的多线程程序的内存一致性做了如下保证。如果程序是正确同步的，程序的执行将具有顺序一致性（Sequentially Consistent）——即程序的执行结果与该程序在顺序一致性内存模型中的执行结果相同。马上我们就会看到，这对于程序员来说是一个极强的保证。这里的同步是指广义上的同步，包括对常用同步原语（synchronized、volatile和final）的正确使用。  

##### 顺序一致性内存模型

顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型，它为程序员提供了极强的内存可见性保证。顺序一致性内存模型有两大特性。

1. 一个线程中的所有操作必须按照程序的顺序来执行。
2. （不管程序是否同步）所有线程都只能看到一个单一的操作执行顺序。在顺序一致性内存模型中，每个操作都必须原子执行且立刻对所有线程可见。  

顺序一致性内存模型为程序员提供的视图如下图  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511173917.png)

在概念上，顺序一致性模型有一个单一的全局内存，这个内存通过一个左右摆动的开关可以连接到任意一个线程，同时每一个线程必须按照程序的顺序来执行内存读/写操作。从上面图可以看出，在任意时间点最多只能有一个线程可以连接到内存。  这样把所有线程的所有内存读/写操作串行化。

假设有两个线程A和B并发执行。其中A线程有3个操作，它们在程序中的顺序是：A1→A2→A3。B线程也有3个操作，它们在程序中的顺序是：B1→B2→B3。  

假设这两个线程使用监视器锁来正确同步：A线程的3个操作执行后释放监视器锁，随后B线程获取同一个监视器锁。那么程序在顺序一致性模型中的执行效果将如下图示。  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511174137.png)

再假设这两个线程没有做同步，下面是这个未同步程序在顺序一致性模型中的执行示意图  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511174210.png)

未同步程序在顺序一致性模型中虽然整体执行顺序是无序的，但所有线程都只能看到一个一致的整体执行顺序。以上图为例，线程A和B看到的执行顺序都是：
B1→A1→A2→B2→A3→B3。之所以能得到这个保证是因为顺序一致性内存模型中的每个操作必须立即对任意线程可见。  

在JMM中就没有这个保证。未同步程序在JMM中不但整体的执行顺序是无序的，而且所有线程看到的操作执行顺序也可能不一致。比如，当前线程写过的数据缓存在本地内存中，并没有刷新到主内存之前，这个写操作只对当前线程可见；从其他线程角度来看，这个写操作根本就没有执行。只有当前线程把本地内存写过的数据刷新到主内存中后，这个操作对其他线程才可见。这个时候，每个线程看到的执行顺序就不一致了。

##### 同步程序的顺序一致性结果

```java
class SynchronizedExample {
	int a = 0;
	boolean flag = false;
	public synchronized void writer() { // 获取锁
		a = 1;
		flag = true;
	}									// 释放锁
	public synchronized void reader() { // 获取锁
		if (flag) {
			int i = a;
		}								// 释放锁
	}
}
```

假设A线程执行writer()方法后，B线程执行reader()方法。这是一个正确同步的多线程程序。根据JMM规范，该程序的执行结果将与该程序在顺序一致性模型中的执行结果相同。  

顺序一致性模型中，所有操作完全按程序的顺序串行执行。而在JMM中，临界区内的代码可以重排序（但JMM不允许临界区内的代码“逸出”到临界区之外，那样会破坏监视器的语义）。JMM会在退出临界区和进入临界区这两个关键时间点做一些特别处理，使得线程在这两个时间点具有与顺序一致性模型相同的内存视图。虽然线程A在临界区内做了重排序，但由于监视器互斥执行的特性，这里的线程B根本无法“观察”到线程A在临界区内的重排序。这种重排序既提高了执行效率，又没有改变程序的执行结果。  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511175036.png)

JMM在具体实现上的基本方针为：在不改变（正确同步的）程序执行结果的前提下，尽可能地为编译器和处理器的优化打开方便之门。  

##### 未同步程序的执行特性

对于未同步或未正确同步的多线程程序，JMM只提供最小安全性：线程执行时读取到的值，要么是之前某个线程写入的值，要么是默认值（0，Null，False），JMM保证线程读操作读取到的值不会无中生有（Out Of Thin Air）的冒出来。为了实现最小安全性，JVM在堆上分配对象时，首先会对内存空间进行清零，然后才会在上面分配对象（JVM内部会同步这两个操作）。因此，在已清零的内存空间（Pre-zeroed Memory）分配对象时，域的默认初始化已经完成了 。

JMM不保证未同步程序的执行结果与该程序在顺序一致性模型中的执行结果一致。因为如果想要保证执行结果一致，JMM需要禁止大量的处理器和编译器的优化，这对程序的执行性能会产生很大的影响。而且，未同步程序在顺序一致性模型中，整体是无序的且结果无法预知，所以保证未同步程序在两个模型中执行结果一致也没什么意义。

未同步程序在两个模型中的执行特性有如下几个差异：

1. 顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。
2. 顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看到一致的操作执行顺序  
3. JMM不保证对64位的long型和double型变量的写操作具有原子性，而顺序一致性模型保证对所有的内存读/写操作都具有原子性。  

在计算机中，数据通过总线在处理器和内存之间传递。每次处理器和内存之间的数据传递都是通过一系列步骤来完成的，这一系列步骤称之为总线事务（Bus Transaction）。总线事务包括读事务（Read Transaction）和写事务（Write Transaction）。读事务从内存传送数据到处理器，写事务从处理器传送数据到内存，每个事务会读/写内存中一个或多个物理上连续的字。这里的关键是，总线会同步试图并发使用总线的事务。在一个处理器执行总线事务期间，总线会禁止其他的处理器和I/O设备执行内存的读/写。  

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511180405.png)

假设处理器A，B和C同时向总线发起总线事务，这时总线仲裁（Bus Arbitration）会对竞争做出裁决，这里假设总线在仲裁后判定处理器A在竞争中获胜（总线仲裁会确保所有处理器都能公平的访问内存）。此时处理器A继续它的总线事务，而其他两个处理器则要等待处理器A的总线事务完成后才能再次执行内存访问。假设在处理器A执行总线事务期间（不管这个总线事务是读事务还是写事务），处理器D向总线发起了总线事务，此时处理器D的请求
会被总线禁止。  

总线的这些工作机制可以把所有处理器对内存的访问以串行化的方式来执行。在任意时间点，最多只能有一个处理器可以访问内存。这个特性确保了单个总线事务之中的内存读/写操作具有原子性。  

在一些32位的处理器上，如果要求对64位数据的写操作具有原子性，会有比较大的开销。为了照顾这种处理器，Java语言规范鼓励但不强求JVM对64位的long型变量和double型变量的写操作具有原子性。当JVM在这种处理器上运行时，可能会把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行。这两个32位的写操作可能会被分配到不同的总线事务中执行，此时对这个64位变量的写操作将不具有原子性。  

当单个内存操作不具有原子性时，可能会产生意想不到后果。如下图

![](https://raw.githubusercontent.com/big-white-2020/notes-image/master/img/20200511180619.png)

如上图，假如一个处理器A写一个Long整型变量，64位写操作被分成两个32位的写操作，且分配到不同事务上执行。同时处理器B的64位读操作被分配到同一个事务中执行。那么B只能读到A写了一半的无效值。

从 JDK5 开始只允许把一个64位long/double型变量的写操作拆分为两个32位的写操作来执行，任意的读操作都必须具有原子性（即任意读操作必须要在单个读事务中执行）。